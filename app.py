# app.py  â€” æ”¯æ´ 15 ç§’ç›£çœ‹ï¼ˆCloud Tasks æ‰‡å‡º 0/15/30/45ï¼‰ï¼Œä¸¦é¡¯ç¤ºæ´»å‹•åç¨±/åœ°é»/æ—¥æœŸ/åœ–ç‰‡
import base64
import hashlib
import hmac
import json
import logging
import os
import re
import secrets
import time
from typing import Dict, Tuple, Optional
from urllib.parse import urlparse, urljoin

import requests
from bs4 import BeautifulSoup
from flask import Flask, abort, jsonify, request
from google.cloud import firestore

app = Flask(__name__)
logging.basicConfig(level=os.getenv("LOG_LEVEL", "INFO"))

# ---------- åŸºç¤è¨­å®š ----------
db = firestore.Client()
LINE_TOKEN = os.environ.get("LINE_CHANNEL_ACCESS_TOKEN") or os.environ.get("LINE_TOKEN")
LINE_SECRET = os.environ.get("LINE_CHANNEL_SECRET") or os.environ.get("LINE_SECRET")

# é è¨­ 60ï¼Œæœ€ä½é™åˆ¶æˆ‘å€‘æœƒå¼·åˆ¶æˆ 15 ç§’
DEFAULT_PERIOD_SEC = int(os.environ.get("DEFAULT_PERIOD_SEC", "60"))
MAX_TASKS_PER_TICK = int(os.environ.get("MAX_TASKS_PER_TICK", "25"))

# Cloud Tasks æ‰‡å‡ºï¼ˆè¦ 15 ç§’å¿…å‚™ï¼‰
PROJECT_ID = os.environ.get("GOOGLE_CLOUD_PROJECT")
TASKS_QUEUE = os.environ.get("TASKS_QUEUE", "tick-queue")
TASKS_LOCATION = os.environ.get("TASKS_LOCATION", "asia-east1")
# ç”¨ä¾†ç°½ OIDC token å‘¼å« Cloud Run çš„ SAï¼ˆè¦æœ‰ roles/run.invokerï¼‰
TASKS_SERVICE_ACCOUNT = os.environ.get("TASKS_SERVICE_ACCOUNT", "")
# ä½ çš„ Cloud Run æœå‹™æ ¹ç¶²å€ï¼ˆä¸å«è·¯å¾‘ï¼‰ï¼Œä¾‹ï¼š https://ticketsearch-xxxx-asia-east1.run.app
TASKS_TARGET_URL = os.environ.get("TASKS_TARGET_URL", "")
# æ˜¯å¦å•Ÿç”¨æ‰‡å‡ºï¼ˆ1=å•Ÿç”¨ï¼›0=ä¸å•Ÿç”¨ï¼Œç›´æ¥æ¯åˆ†é˜è·‘ä¸€æ¬¡ï¼‰
TICK_FANOUT = os.environ.get("TICK_FANOUT", "1") == "1"

UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/126.0 Safari/537.36"
)

# æ–‡å­—åµæ¸¬è¦å‰‡ï¼ˆåŠ å…¥ã€Œç©ºä½ã€ï¼‰
_RE_QTY = re.compile(r"(ç©ºä½|å‰©é¤˜|å°šé¤˜|å°šæœ‰|å¯å”®|é¤˜ç¥¨|åé¡)[^\d]{0,5}(\d+)", re.I)
_RE_SOLDOUT = re.compile(r"(å”®ç½„|å®Œå”®|ç„¡ç¥¨|å·²å”®å®Œ|æš«ç„¡|æš«æ™‚ç„¡|å”®å®Œ|å·²å”®ç©º)", re.I)

HELP_TEXT = (
    "æˆ‘æ˜¯ç¥¨åˆ¸ç›£çœ‹æ©Ÿå™¨äºº ğŸ‘‹\n"
    "æŒ‡ä»¤ï¼š\n"
    "/start æˆ– /help ï¼ é¡¯ç¤ºé€™å€‹èªªæ˜\n"
    "/watch <URL> [ç§’] ï¼ é–‹å§‹ç›£çœ‹ï¼ˆæœ€å° 15 ç§’ï¼‰\n"
    "/unwatch <ä»»å‹™ID> ï¼ åœç”¨ä»»å‹™\n"
    "/list ï¼ æŸ¥çœ‹æœ€è¿‘ä»»å‹™"
)

# ---------- LINE ----------
def _line_reply(reply_token: str, text: str) -> None:
    if not LINE_TOKEN or not reply_token:
        app.logger.warning("No LINE_TOKEN or reply_token; skip reply")
        return
    url = "https://api.line.me/v2/bot/message/reply"
    headers = {"Authorization": f"Bearer {LINE_TOKEN}", "Content-Type": "application/json"}
    body = {"replyToken": reply_token, "messages": [{"type": "text", "text": text[:4000]}]}
    try:
        r = requests.post(url, headers=headers, json=body, timeout=10)
        r.raise_for_status()
    except Exception as e:
        app.logger.exception(f"LINE reply failed: {e}")


def _line_push_text(to: str, text: str) -> None:
    if not LINE_TOKEN or not to:
        app.logger.warning("No LINE_TOKEN or target; skip push")
        return
    url = "https://api.line.me/v2/bot/message/push"
    headers = {"Authorization": f"Bearer {LINE_TOKEN}", "Content-Type": "application/json"}
    body = {"to": to, "messages": [{"type": "text", "text": text[:4000]}]}
    try:
        r = requests.post(url, headers=headers, json=body, timeout=10)
        r.raise_for_status()
    except Exception as e:
        app.logger.exception(f"LINE push failed: {e}")


def _line_push_rich(to: str, text: str, image_url: Optional[str] = None) -> None:
    """æœ‰åœ–å°±å…ˆé€åœ–ç‰‡ï¼Œå†é€æ–‡å­—"""
    if not LINE_TOKEN or not to:
        app.logger.warning("No LINE_TOKEN or target; skip push")
        return
    url = "https://api.line.me/v2/bot/message/push"
    headers = {"Authorization": f"Bearer {LINE_TOKEN}", "Content-Type": "application/json"}
    messages = []
    if image_url:
        messages.append({
            "type": "image",
            "originalContentUrl": image_url,
            "previewImageUrl": image_url
        })
    messages.append({"type": "text", "text": text[:4000]})
    try:
        r = requests.post(url, headers=headers, json={"to": to, "messages": messages}, timeout=10)
        r.raise_for_status()
    except Exception as e:
        app.logger.exception(f"LINE push (rich) failed: {e}")


def _verify_line_signature(raw_body: bytes) -> bool:
    if not LINE_SECRET:
        return True
    sig = request.headers.get("X-Line-Signature", "")
    digest = hmac.new(LINE_SECRET.encode("utf-8"), raw_body, hashlib.sha256).digest()
    expected = base64.b64encode(digest).decode("utf-8")
    return hmac.compare_digest(sig, expected)

# ---------- ibon è§£æ ----------
def _req_session() -> requests.Session:
    s = requests.Session()
    s.headers.update({
        "User-Agent": UA,
        "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8",
        "Cache-Control": "no-cache",
    })
    return s


def resolve_ibon_orders_url(any_url: str) -> Optional[str]:
    """å›å‚³ orders.ibon ä¸‹å–®é ï¼ˆUTK0201_xxx.aspxï¼‰"""
    u = urlparse(any_url)
    if "orders.ibon.com.tw" in u.netloc and "UTK0201" in u.path.upper():
        return any_url
    if "ticket.ibon.com.tw" in u.netloc:
        s = _req_session()
        r = s.get(any_url, timeout=15)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")
        a = soup.select_one('a[href*="orders.ibon.com.tw"][href*="UTK02"][href*="UTK0201"]')
        if a and a.get("href"):
            return urljoin(any_url, a["href"])
        for tag in soup.find_all(["a", "button"]):
            href = (tag.get("href") or tag.get("data-url") or "").strip()
            if "orders.ibon.com.tw" in href and "UTK0201" in href.upper():
                return urljoin(any_url, href)
    return None


def _first_text(*cands: Optional[str]) -> str:
    for c in cands:
        if c and str(c).strip():
            return str(c).strip()
    return ""


def _extract_activity_meta(soup: BeautifulSoup) -> Dict[str, str]:
    """
    å˜—è©¦å¾ UTK0201 é é¢æŠ“ã€Œæ´»å‹•åç¨±/åœ°é»/æ—¥æœŸ/åœ–ç‰‡ã€ã€‚
    1) å…ˆæ‰¾å›ºå®šæ¨™ç±¤æ–‡å­—ï¼ˆæ´»å‹•åç¨±/æ´»å‹•æ™‚é–“/æ´»å‹•åœ°é»ï¼‰
    2) é€€å› og:meta æˆ–é é¢é¦–å¼µæ´»å‹•åœ–
    """
    def find_label(labels):
        pat = re.compile("|".join(map(re.escape, labels)))
        for node in soup.find_all(text=pat):
            tag = node.parent
            # è¡¨æ ¼ï¼šæ¨™ç±¤åœ¨ th/tdï¼Œå€¼åœ¨åŒä¸€åˆ—çš„ä¸‹ä¸€å€‹å„²å­˜æ ¼
            if tag and tag.name in ("td", "th"):
                cells = [c.get_text(" ", strip=True) for c in tag.parent.find_all(["td", "th"])]
                for i, val in enumerate(cells):
                    if re.search(pat, val) and i + 1 < len(cells):
                        return cells[i + 1]
            # ä¸€èˆ¬ <li>/<div>ï¼šåŒå…ƒç´ æ–‡å­—å«æ¨™ç±¤ï¼Œå»é™¤æ¨™ç±¤èˆ‡å†’è™Ÿ
            text = tag.get_text(" ", strip=True) if tag else ""
            if re.search(pat, text):
                cleaned = pat.sub("", text).replace("ï¼š", "").strip()
                if cleaned:
                    return cleaned
            # å˜—è©¦æ‰¾ä¸‹ä¸€å€‹æœ‰å­—çš„å…„å¼Ÿå…ƒç´ 
            sib = tag.find_next_sibling() if tag else None
            if sib:
                t = sib.get_text(" ", strip=True)
                if t:
                    return t
        return ""

    title = _first_text(
        find_label(["æ´»å‹•åç¨±"]),
        soup.select_one('meta[property="og:title"]') and soup.select_one('meta[property="og:title"]').get("content"),
    )
    dt = _first_text(
        find_label(["æ´»å‹•æ™‚é–“", "æ´»å‹•æ—¥æœŸ"]),
    )
    venue = _first_text(
        find_label(["æ´»å‹•åœ°é»", "åœ°é»"]),
    )

    # åœ–ç‰‡ï¼šå…ˆæ‰¾ og:imageï¼Œå†æ‰¾é é¢ä¸Šçš„æ´»å‹•åœ–
    image = _first_text(
        soup.select_one('meta[property="og:image"]') and soup.select_one('meta[property="og:image"]').get("content"),
    )
    if not image:
        img = soup.find("img", src=re.compile(r"ActivityImage|azureedge|image/ActivityImage", re.I))
        if img and img.get("src"):
            image = urljoin("https://orders.ibon.com.tw/", img.get("src"))

    return {"title": title, "datetime": dt, "venue": venue, "image_url": image}


def parse_ibon_orders_static(html: str) -> Dict:
    """
    è§£æå„å€å¯å”®å¼µæ•¸ï¼›ä¸¦å›å‚³ soldout æç¤ºã€‚
    """
    soup = BeautifulSoup(html, "html.parser")
    sections: Dict[str, int] = {}
    total = 0
    soldout_hint = False

    # A) è§£ææœ‰ã€Œç©ºä½/å‰©é¤˜ã€æ¬„ä½çš„è¡¨æ ¼
    for table in soup.find_all("table"):
        header_tr = None
        for tr in table.find_all("tr", recursive=True):
            if tr.find("th"):
                heads = [c.get_text(" ", strip=True) for c in tr.find_all(["th", "td"])]
                if any(h for h in heads if ("ç©ºä½" in h or "å‰©é¤˜" in h or "å¯å”®" in h)):
                    header_tr = tr
                    break
        if not header_tr:
            continue

        heads = [c.get_text(" ", strip=True) for c in header_tr.find_all(["th", "td"])]
        try:
            idx_qty = next(i for i, h in enumerate(heads) if ("ç©ºä½" in h or "å‰©é¤˜" in h or "å¯å”®" in h))
        except StopIteration:
            continue
        try:
            idx_area = next(i for i, h in enumerate(heads) if ("ç¥¨å€" in h or h == "å€" or "åº§ä½å€" in h))
        except StopIteration:
            idx_area = 1 if len(heads) > 1 else 0

        tr = header_tr.find_next_sibling("tr")
        while tr and tr.name == "tr":
            tds = tr.find_all("td")
            if len(tds) > max(idx_qty, idx_area):
                area = tds[idx_area].get_text(" ", strip=True)
                qty_text = tds[idx_qty].get_text(" ", strip=True)
                if _RE_SOLDOUT.search(qty_text):
                    soldout_hint = True
                m = re.search(r"(\d+)", qty_text)
                if m:
                    qty = int(m.group(1))
                    if qty > 0:
                        area = re.sub(r"\s+", "", area) or "æœªå‘½åå€"
                        sections[area] = sections.get(area, 0) + qty
                        total += qty
            tr = tr.find_next_sibling("tr")
        if total > 0:
            break

    # B) é—œéµå­—æƒæï¼ˆå«ã€Œç©ºä½ã€ï¼‰
    if total == 0:
        candidates = soup.select("tr, li, div, p, span")
        for node in candidates:
            txt = node.get_text(" ", strip=True)
            if not txt:
                continue
            if _RE_SOLDOUT.search(txt):
                soldout_hint = True
            m = _RE_QTY.search(txt)
            if not m:
                continue
            qty = int(m.group(2))
            if qty <= 0:
                continue

            # å„ªå…ˆå¾åŒä¸€åˆ—(tr)æ‰¾å€å
            key = "æœªå‘½åå€"
            tr = node if node.name == "tr" else node.find_parent("tr")
            if tr:
                cells = [c.get_text(" ", strip=True) for c in tr.find_all(["td", "th"])]
                for c in cells:
                    if re.search(r"(å€|çœ‹å°|å…§é‡|å¤–é‡|åº§|æ¨“|å±¤)", c):
                        key = re.sub(r"\s+", "", c)
                        break
            sections[key] = sections.get(key, 0) + qty
            total += qty

    return {"sections": sections, "total": total, "soldout": (total == 0 and soldout_hint), "soup": soup}


def check_ibon(any_url: str) -> Tuple[bool, str, str, Dict[str, str]]:
    """
    å›å‚³: (ok, message, signature, meta)
    meta: {"title","venue","datetime","image_url"}
    """
    orders_url = resolve_ibon_orders_url(any_url)
    if not orders_url:
        return False, "æ‰¾ä¸åˆ° ibon ä¸‹å–®é ï¼ˆUTK0201ï¼‰ã€‚å¯èƒ½å°šæœªé–‹è³£æˆ–æŒ‰éˆ•æœªé¡¯ç¤ºã€‚", "NA", {}

    s = _req_session()
    r = s.get(orders_url, timeout=15)
    r.raise_for_status()

    parsed = parse_ibon_orders_static(r.text)
    meta = _extract_activity_meta(parsed["soup"])
    title = meta.get("title") or "æ´»å‹•è³‡è¨Š"
    venue = meta.get("venue")
    dt = meta.get("datetime")
    img = meta.get("image_url")

    prefix_lines = [f"ğŸ« {title}"]
    if venue:
        prefix_lines.append(f"åœ°é»ï¼š{venue}")
    if dt:
        prefix_lines.append(f"æ—¥æœŸï¼š{dt}")
    prefix = "\n".join(prefix_lines) + "\n\n"

    if parsed["total"] > 0:
        parts = [f"{k}: {v} å¼µ" for k, v in sorted(parsed["sections"].items(), key=lambda kv: (-kv[1], kv[0]))]
        msg = prefix + "âœ… ç›£çœ‹çµæœï¼šç›®å‰å¯å”®\n" + "\n".join(parts) + f"\nåˆè¨ˆï¼š{parsed['total']} å¼µ\n{orders_url}"
        sig = hashlib.md5(("|" + "|".join(parts)).encode()).hexdigest()
        return True, msg, sig, {"image_url": img}

    if parsed.get("soldout"):
        msg = prefix + f"ç›®å‰é¡¯ç¤ºå®Œå”®/ç„¡ç¥¨\n{orders_url}"
        sig = hashlib.md5("soldout".encode()).hexdigest()
        return True, msg, sig, {"image_url": img}

    return False, prefix + "æš«æ™‚è®€ä¸åˆ°å‰©é¤˜æ•¸ï¼ˆå¯èƒ½ç‚ºå‹•æ…‹è¼‰å…¥ï¼‰ã€‚\n" + orders_url, "NA", {"image_url": img}

# ---------- Webhook ----------
@app.post("/webhook")
def webhook():
    raw = request.get_data()
    if not _verify_line_signature(raw):
        app.logger.error("Invalid LINE signature")
        return "bad signature", 400

    body = request.get_json(silent=True) or {}
    events = body.get("events", [])

    for ev in events:
        etype = ev.get("type")

        if etype in ("follow", "join"):
            _line_reply(ev.get("replyToken"), HELP_TEXT)
            continue

        if etype != "message":
            continue

        msg = ev.get("message", {})
        if msg.get("type") != "text":
            continue

        text = (msg.get("text") or "").strip()
        reply_token = ev.get("replyToken")
        src = ev.get("source", {})

        if text.lower() in ("/start", "start", "/help", "help", "ï¼Ÿ"):
            _line_reply(reply_token, HELP_TEXT)
            continue

        if text.lower().startswith("/watch"):
            parts = text.split()
            if len(parts) < 2:
                _line_reply(reply_token, "ç”¨æ³•ï¼š/watch <ç¥¨åˆ¸ç¶²å€>\nå¯è²¼æ´»å‹•é æˆ– orders å…§é ")
                continue
            url = parts[1]
            period = DEFAULT_PERIOD_SEC
            if len(parts) >= 3:
                try:
                    p = int(parts[2])
                    period = max(15, min(3600, p))  # â˜… æœ€å° 15 ç§’
                except Exception:
                    pass

            target_id = src.get("userId") or src.get("groupId") or src.get("roomId")
            target_type = "user" if src.get("userId") else ("group" if src.get("groupId") else "room")

            task_id = secrets.token_urlsafe(4)
            now = int(time.time())
            db.collection("watches").document(task_id).set({
                "url": url,
                "targetType": target_type,
                "targetId": target_id,
                "periodSec": period,
                "nextCheckAt": now,     # ç«‹åˆ» due
                "lastSig": None,
                "active": True,
                "createdAt": now,
            })
            _line_reply(
                reply_token,
                f"å·²é–‹å§‹ç›£çœ‹ âœ…\nä»»å‹™IDï¼š{task_id}\næ¯ {period} ç§’æª¢æŸ¥ä¸€æ¬¡\nURLï¼š{url}"
            )
            continue

        if text.lower().startswith("/unwatch"):
            parts = text.split()
            if len(parts) < 2:
                _line_reply(reply_token, "ç”¨æ³•ï¼š/unwatch <ä»»å‹™ID>")
                continue
            tid = parts[1]
            doc = db.collection("watches").document(tid)
            if doc.get().exists:
                doc.update({"active": False})
                _line_reply(reply_token, f"ä»»å‹™ {tid} å·²åœç”¨")
            else:
                _line_reply(reply_token, f"æ‰¾ä¸åˆ°ä»»å‹™ {tid}")
            continue

        if text.lower().startswith("/list"):
            target_id = src.get("userId") or src.get("groupId") or src.get("roomId")
            q = (db.collection("watches")
                 .where("targetId", "==", target_id)
                 .order_by("createdAt", direction=firestore.Query.DESCENDING)
                 .limit(10))
            docs = list(q.stream())
            if not docs:
                _line_reply(reply_token, "ç›®å‰æ²’æœ‰ä»»å‹™")
            else:
                lines = []
                for d in docs:
                    x = d.to_dict()
                    flag = "å•Ÿç”¨" if x.get("active") else "åœç”¨"
                    lines.append(f"{d.id}ï½œ{flag}ï½œ{x.get('periodSec', 60)}s\n{x.get('url')}")
                _line_reply(reply_token, "ä½ çš„ä»»å‹™ï¼š\n" + "\n\n".join(lines))
            continue

        _line_reply(reply_token, HELP_TEXT)

    return "OK"

# ---------- 15 ç§’æ‰‡å‡ºï¼šCloud Tasks ----------
def enqueue_tick_runs(delays=(0, 15, 30, 45)) -> int:
    """å»ºç«‹ Cloud Tasks åœ¨ 0/15/30/45 ç§’å‘¼å« /cron/tick?mode=run"""
    try:
        from google.cloud import tasks_v2
        from google.protobuf import timestamp_pb2
    except Exception as e:
        app.logger.warning(f"[fanout] cloud-tasks lib missing, run once. {e}")
        return 0

    if not (PROJECT_ID and TASKS_QUEUE and TASKS_LOCATION and TASKS_SERVICE_ACCOUNT and TASKS_TARGET_URL):
        app.logger.warning("[fanout] env incomplete; run once instead.")
        return 0

    client = tasks_v2.CloudTasksClient()
    parent = client.queue_path(PROJECT_ID, TASKS_LOCATION, TASKS_QUEUE)
    created = 0

    for d in delays:
        ts = timestamp_pb2.Timestamp()
        ts.FromSeconds(int(time.time()) + int(d))
        task = {
            "http_request": {
                "http_method": tasks_v2.HttpMethod.GET,
                "url": f"{TASKS_TARGET_URL}/cron/tick?mode=run",
                "headers": {"User-Agent": "Cloud-Tasks", "X-From-Tasks": "1"},
                "oidc_token": {
                    "service_account_email": TASKS_SERVICE_ACCOUNT,
                    "audience": TASKS_TARGET_URL
                },
            },
            "schedule_time": ts
        }
        try:
            client.create_task(request={"parent": parent, "task": task})
            created += 1
        except Exception as e:
            app.logger.exception(f"[fanout] create_task failed (delay={d}): {e}")

    return created


def do_tick():
    """çœŸæ­£åŸ·è¡Œä¸€æ¬¡æª¢æŸ¥ï¼ˆåŸæœ¬ /cron/tick çš„é‚è¼¯ï¼‰"""
    now = int(time.time())
    q = (db.collection("watches")
         .where("active", "==", True)
         .where("nextCheckAt", "<=", now)
         .limit(MAX_TASKS_PER_TICK))
    try:
        docs = list(q.stream())
    except Exception as e:
        app.logger.exception(f"[tick] Firestore query failed: {e}")
        return jsonify({"ok": False, "stage": "query", "error": str(e)}), 200

    processed, errors = 0, []
    for d in docs:
        task = d.to_dict()
        try:
            ok, msg, sig, meta = check_ibon(task["url"])
            if ok and sig != task.get("lastSig"):
                _line_push_rich(task["targetId"], msg, (meta or {}).get("image_url"))
                d.reference.update({"lastSig": sig})
        except Exception as e:
            app.logger.exception(f"[tick] task {d.id} failed: {e}")
            errors.append(f"{d.id}:{type(e).__name__}")
        finally:
            period = int(task.get("periodSec", DEFAULT_PERIOD_SEC))
            d.reference.update({"nextCheckAt": now + max(15, period)})  # â˜… æœ€å° 15 ç§’
            processed += 1

    return jsonify({"ok": True, "processed": processed, "due": len(docs), "errors": errors, "ts": now}), 200


@app.get("/cron/tick")
def cron_tick():
    # mode=runï¼šè¢« Cloud Tasks å‘¼å« â†’ ç›´æ¥åŸ·è¡Œ
    if request.args.get("mode") == "run" or request.headers.get("X-From-Tasks") == "1":
        return do_tick()

    # Scheduler é€²ä¾†ï¼ˆæ¯åˆ†é˜ä¸€æ¬¡ï¼‰
    if TICK_FANOUT:
        n = enqueue_tick_runs((0, 15, 30, 45))
        if n > 0:
            return jsonify({"ok": True, "fanout": n}), 200

    # å¾Œå‚™ï¼šè‹¥æ²’æˆåŠŸæ‰‡å‡ºï¼Œå°±åŸ·è¡Œä¸€æ¬¡
    return do_tick()


@app.get("/healthz")
def healthz():
    return "ok", 200


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", "8080")))