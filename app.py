# app.py â€” ibon ç¥¨åˆ¸ç›£çœ‹ï¼ˆLINE Bot on Cloud Runï¼‰
# åŠŸèƒ½ï¼š
# - /check <URL>ï¼šæ‰‹å‹•æŸ¥è©¢ï¼Œå¯è‡ªå‹•æ·±å…¥ 001 ç¥¨å€æŠ“å¯å”®å¼µæ•¸ï¼ˆå« HTML/JSON æ‰«æï¼‰
# - /watch <URL> [ç§’]ï¼šå»ºç«‹ç›£çœ‹ï¼ˆåŒç¶²å€ä¸é‡è¤‡ï¼›å¸¶æ–°ç§’æ•¸æœƒæ›´æ–°ï¼›/unwatch åœç”¨ï¼›/list æª¢è¦–ï¼‰
# - /cron/tickï¼šè¢« Cloud Scheduler å«é†’ï¼Œè™•ç† due ä»»å‹™ä¸¦ç™¼ LINE push
# - åœ–ç‰‡ï¼šæœƒè‡ªå‹•æŠ“æ´»å‹•ä¸»è¦–è¦ºï¼ˆog:image / <img> / regex / ActivityInfo é ï¼‰
# - è£œåŠ©ï¼š/diag?url=... ç›´æ¥çœ‹è§£æçµæœï¼ˆæ¸¬è©¦ä¾¿åˆ©ï¼‰
# å»ºè­°ç’°å¢ƒè®Šæ•¸ï¼š
#   LINE_CHANNEL_ACCESS_TOKEN / LINE_TOKEN
#   LINE_CHANNEL_SECRET / LINE_SECRET
#   GOOGLE_CLOUD_PROJECTï¼ˆCloud Run é è¨­æœƒæœ‰ï¼‰
#   DEFAULT_PERIOD_SEC=60 ï¼ˆé¸å¡«ï¼‰
#   TICK_FANOUT=0 æˆ– 1ï¼ˆé è¨­ 1ï¼›è‹¥ä¸ä½¿ç”¨ Cloud Tasks æ‰‡å‡ºï¼Œå»ºè­°è¨­ 0ï¼‰
#   ALWAYS_NOTIFY=0 æˆ– 1ï¼ˆé è¨­ 0ï¼›è¨­ 1 è¡¨ç¤ºæ¯æ¬¡æ’ç¨‹éƒ½æ¨æ’­ç¾æ³ï¼‰
#   ï¼Šè‹¥è¦ç”¨ Cloud Tasks æ‰‡å‡ºï¼šTASKS_QUEUE / TASKS_LOCATION / TASKS_SERVICE_ACCOUNT / TASKS_TARGET_URL

import base64
import hashlib
import hmac
import json
import logging
import os
import re
import secrets
import time
from typing import Dict, Tuple, Optional, List, Set
from urllib.parse import urlparse, urljoin, parse_qsl, urlencode, quote

import requests
from bs4 import BeautifulSoup
from flask import Flask, jsonify, request
from google.cloud import firestore

app = Flask(__name__)
logging.basicConfig(level=os.getenv("LOG_LEVEL", "INFO"))

# ---------- ç’°å¢ƒè®Šæ•¸ ----------
db = firestore.Client()
LINE_TOKEN = os.environ.get("LINE_CHANNEL_ACCESS_TOKEN") or os.environ.get("LINE_TOKEN")
LINE_SECRET = os.environ.get("LINE_CHANNEL_SECRET") or os.environ.get("LINE_SECRET")

DEFAULT_PERIOD_SEC = int((os.environ.get("DEFAULT_PERIOD_SEC") or os.environ.get("DEFAULT_INTERVAL") or "60"))
MAX_TASKS_PER_TICK = int(os.environ.get("MAX_TASKS_PER_TICK", "25"))

PROJECT_ID = os.environ.get("GOOGLE_CLOUD_PROJECT")
TASKS_QUEUE = os.environ.get("TASKS_QUEUE", "tick-queue")
TASKS_LOCATION = os.environ.get("TASKS_LOCATION", "asia-east1")
TASKS_SERVICE_ACCOUNT = os.environ.get("TASKS_SERVICE_ACCOUNT", "")
TASKS_TARGET_URL = os.environ.get("TASKS_TARGET_URL", "")
TICK_FANOUT = os.environ.get("TICK_FANOUT", "1") == "1"

ALWAYS_NOTIFY = os.getenv("ALWAYS_NOTIFY", "0") == "1"   # 1=æ¯æ¬¡æ’ç¨‹éƒ½æ¨æ’­ï¼›0=ç‹€æ…‹è®Šå‹•æ™‚æ‰æ¨

UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
      "(KHTML, like Gecko) Chrome/126.0 Safari/537.36")

HELP_TEXT = (
    "æˆ‘æ˜¯ç¥¨åˆ¸ç›£çœ‹æ©Ÿå™¨äºº ğŸ‘‹\n"
    "æŒ‡ä»¤ï¼š\n"
    "/start æˆ– /help ï¼ é¡¯ç¤ºé€™å€‹èªªæ˜\n"
    "/watch <URL> [ç§’] ï¼ é–‹å§‹ç›£çœ‹ï¼ˆåŒç¶²å€ä¸é‡è¤‡ï¼›å¸¶æ–°ç§’æ•¸æœƒæ›´æ–°ï¼›æœ€å° 15 ç§’ï¼‰\n"
    "/unwatch <ä»»å‹™ID> ï¼ åœç”¨ä»»å‹™\n"
    "/list ï¼ é¡¯ç¤ºå•Ÿç”¨ä¸­ä»»å‹™ï¼ˆ/list all çœ‹å…¨éƒ¨ï¼›/list off åªçœ‹åœç”¨ï¼‰\n"
    "/check <URL> ï¼ ç«‹åˆ»æ‰‹å‹•æŸ¥è©¢è©²é å‰©é¤˜ç¥¨æ•¸\n"
    "/checkid <ä»»å‹™ID> ï¼ ç«‹åˆ»æ‰‹å‹•æŸ¥è©¢è©²ä»»å‹™çš„ URL\n"
    "ä¹Ÿå¯è¼¸å…¥ã€ŒæŸ¥è©¢ã€æˆ– /check ä¸å¸¶åƒæ•¸ï¼ŒæœƒæŸ¥ä½ æœ€è¿‘ä¸€ç­†å•Ÿç”¨ä¸­çš„ä»»å‹™"
)

# ---------- è¦å‰‡ ----------
_RE_QTY = re.compile(r"(ç©ºä½|å‰©é¤˜|å°šé¤˜|å°šæœ‰|å¯å”®|é¤˜ç¥¨|åé¡|å¸­ä½|å‰©é¤˜å¼µæ•¸|å‰©é¤˜ç¥¨æ•¸|é¤˜æ•¸|å¯è³¼|å‰©ä¸‹)[^\d]{0,6}(\d+)", re.I)
_RE_SOLDOUT = re.compile(r"(å”®ç½„|å®Œå”®|ç„¡ç¥¨|å·²å”®å®Œ|æš«ç„¡|æš«æ™‚ç„¡|å”®å®Œ|å·²å”®ç©º|ç„¡å¯å”®|ç„¡å‰©é¤˜)", re.I)
_RE_AREANAME_NEAR = re.compile(
    r"(æ–æ»¾.?[A-Z]?\s*å€|æ–æ»¾å€|èº«éšœå¸­|èº«å¿ƒéšœç¤™å¸­|ç„¡éšœç¤™å¸­|å…§é‡|å¤–é‡|[A-Z]\s*å€|[A-Z]\d+\s*å€|çœ‹å°\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]\s*æ¨“|[æ±è¥¿å—åŒ—ä¸Šä¸‹]\s*(?=å€|å±¤)|\S{1,8}å€)",
    re.I
)
_RE_ACTIVITY_IMG = re.compile(r"https?://[^\"'<>]*azureedge\.net/[^\"'<>]*ActivityImage[^\"'<>]*\.(?:jpg|jpeg|png)", re.I)
_RE_ACTIVITY_INFO = re.compile(r"(https?://ticket\.ibon\.com\.tw)?/?ActivityInfo/Details/\d+", re.I)
_DATE_PAT = re.compile(
    r"(\d{4}[./-å¹´]\s*\d{1,2}[./-æœˆ]\s*\d{1,2}"
    r"(?:\s*[ï¼ˆ(]?[ä¸€äºŒä¸‰å››äº”å…­æ—¥å¤©é€±å‘¨MonTueWedThuFriSatSunæ˜ŸæœŸ]{1,3}[)ï¼‰]?)?"
    r"(?:\s*\d{1,2}:\d{2})?"
    r"|"
    r"\d{1,2}\s*æœˆ\s*\d{1,2}\s*æ—¥(?:\s*\d{1,2}:\d{2})?"
    r"|"
    r"\d{1,2}/\d{1,2}(?:\s*\d{1,2}:\d{2})?"
    r")"
)

# ---------- LINE ----------
def _line_reply(reply_token: str, text: str) -> None:
    if not LINE_TOKEN or not reply_token: return
    url = "https://api.line.me/v2/bot/message/reply"
    headers = {"Authorization": f"Bearer {LINE_TOKEN}", "Content-Type": "application/json"}
    body = {"replyToken": reply_token, "messages": [{"type": "text", "text": text[:4000]}]}
    try:
        resp = requests.post(url, headers=headers, json=body, timeout=10)
        if resp.status_code >= 400:
            app.logger.error(f"LINE reply failed {resp.status_code}: {resp.text}")
        resp.raise_for_status()
    except Exception as e:
        app.logger.exception(f"LINE reply failed: {e}")

def _line_reply_rich(reply_token: str, text: str, image_url: Optional[str] = None) -> None:
    if not LINE_TOKEN or not reply_token: return
    url = "https://api.line.me/v2/bot/message/reply"
    headers = {"Authorization": f"Bearer {LINE_TOKEN}", "Content-Type": "application/json"}
    messages = []
    if image_url:
        messages.append({"type": "image","originalContentUrl": image_url,"previewImageUrl": image_url})
    messages.append({"type": "text", "text": text[:4000]})
    try:
        resp = requests.post(url, headers=headers, json={"replyToken": reply_token, "messages": messages}, timeout=10)
        if resp.status_code >= 400:
            app.logger.error(f"LINE reply (rich) failed {resp.status_code}: {resp.text}")
        resp.raise_for_status()
    except Exception as e:
        app.logger.exception(f"LINE reply (rich) failed: {e}")

def _line_push_rich(to: str, text: str, image_url: Optional[str] = None) -> None:
    if not LINE_TOKEN or not to: return
    url = "https://api.line.me/v2/bot/message/push"
    headers = {"Authorization": f"Bearer {LINE_TOKEN}", "Content-Type": "application/json"}
    messages = []
    if image_url:
        messages.append({"type": "image","originalContentUrl": image_url,"previewImageUrl": image_url})
    messages.append({"type": "text", "text": text[:4000]})
    try:
        resp = requests.post(url, headers=headers, json={"to": to, "messages": messages}, timeout=10)
        if resp.status_code >= 400:
            app.logger.error(f"LINE push failed {resp.status_code}: {resp.text}")
        resp.raise_for_status()
    except Exception as e:
        app.logger.exception(f"LINE push (rich) failed: {e}")

def _verify_line_signature(raw_body: bytes) -> bool:
    if not LINE_SECRET: return True
    sig = request.headers.get("X-Line-Signature", "")
    digest = hmac.new(LINE_SECRET.encode("utf-8"), raw_body, hashlib.sha256).digest()
    expected = base64.b64encode(digest).decode("utf-8")
    return hmac.compare_digest(sig, expected)

# ---------- å·¥å…· ----------
def _req_session() -> requests.Session:
    s = requests.Session()
    s.headers.update({"User-Agent": UA, "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8", "Cache-Control": "no-cache"})
    return s

def _strip_scripts(soup: BeautifulSoup) -> None:
    for tag in soup.find_all(["script", "style", "noscript"]):
        tag.decompose()

def _clean_text(s: str) -> str:
    s = re.sub(r"\s+", " ", s or "").strip()
    if re.search(r"\b(var|function|\$\(window|if\s*\(|for\s*\(|while\s*\(|\{|\}|;)\b", s):
        return ""
    return s[:120]

def _only_date_like(s: str) -> str:
    s = _clean_text(s)
    if not s: return ""
    m = _DATE_PAT.search(s)
    return m.group(0) if m else ""

# ---------- URL æ•´ç† ----------
def resolve_ibon_orders_url(any_url: str) -> Optional[str]:
    u = urlparse(any_url)
    if "orders.ibon.com.tw" in u.netloc and "UTK0201" in u.path.upper():
        return any_url
    if "ticket.ibon.com.tw" in u.netloc:
        s = _req_session()
        r = s.get(any_url, timeout=15); r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")
        _strip_scripts(soup)
        a = soup.select_one('a[href*="orders.ibon.com.tw"][href*="UTK02"][href*="UTK0201"]')
        if a and a.get("href"):
            return urljoin(any_url, a["href"])
        for tag in soup.find_all(["a", "button"]):
            href = (tag.get("href") or tag.get("data-url") or "").strip()
            if "orders.ibon.com.tw" in href and "UTK0201" in href.upper():
                return urljoin(any_url, href)
    return None

def canonicalize_ibon_url(any_url: str) -> str:
    url = resolve_ibon_orders_url(any_url) or any_url
    p = urlparse(url)
    host = p.netloc.lower()
    path = re.sub(r"/+", "/", p.path)
    pairs = parse_qsl(p.query, keep_blank_values=True)

    keep = {"PERFORMANCE_ID", "PRODUCT_ID"}
    skip = {"STRITM", "UTM_SOURCE", "UTM_MEDIUM", "UTM_CAMPAIGN", "UTM_ID", "UTM_TERM", "UTM_CONTENT", "REF"}

    kept = []
    if "ibon.com.tw" in host and "UTK02" in path.upper():
        for k, v in pairs:
            ku = k.upper()
            if ku in keep:
                kept.append((ku, v.strip()))
        kept.sort()
        q = "&".join(f"{k}={v}" for k, v in kept)
    else:
        for k, v in pairs:
            if k.upper() in skip: continue
            kept.append((k, v))
        kept.sort()
        q = urlencode(kept, doseq=True)

    canon = f"https://{host}{path}"
    if q: canon += "?" + q
    return canon

# ---------- 000ï¼šåŸºæœ¬è³‡è¨Š/éœæ…‹æ•¸é‡ ----------
def _extract_activity_meta(soup: BeautifulSoup) -> Dict[str, str]:
    _strip_scripts(soup)
    def find_label(labels):
        pat = re.compile("|".join(map(re.escape, labels)))
        for node in soup.find_all(string=pat):
            parent = node.parent
            if not parent or parent.name in ("script", "style", "noscript"): continue
            if parent.name in ("td", "th") and parent.parent:
                cells = [c.get_text(" ", strip=True) for c in parent.parent.find_all(["td", "th"])]
                for i, val in enumerate(cells):
                    if re.search(pat, val) and i + 1 < len(cells):
                        out = _clean_text(cells[i + 1])
                        if out: return out
            txt = parent.get_text(" ", strip=True)
            if re.search(pat, txt):
                out = _clean_text(pat.sub("", txt).replace("ï¼š", " ").strip())
                if out: return out
            sib = parent.find_next_sibling()
            if sib:
                out = _clean_text(sib.get_text(" ", strip=True))
                if out: return out
        return ""
    title = _clean_text(find_label(["æ´»å‹•åç¨±"]) or (soup.select_one('meta[property="og:title"]') and soup.select_one('meta[property="og:title"]').get("content", "")))
    dt = _only_date_like(find_label(["æ´»å‹•æ™‚é–“", "æ¼”å‡ºæ™‚é–“"])) or _only_date_like(soup.get_text(" ", strip=True))
    venue = _clean_text(find_label(["æ´»å‹•åœ°é»", "åœ°é»", "å ´é¤¨", "åœ°é»/å ´é¤¨"]))
    return {"title": title, "datetime": dt, "venue": venue}

def _guess_area_from_text(txt: str) -> str:
    m = _RE_AREANAME_NEAR.search(txt or "")
    if not m: return "æœªå‘½åå€"
    return re.sub(r"\s+", "", m.group(0))

def parse_ibon_orders_static(html: str) -> Dict:
    soup = BeautifulSoup(html, "html.parser")
    _strip_scripts(soup)
    sections: Dict[str, int] = {}
    total = 0
    soldout_hint = False

    # A) è¡¨æ ¼æ¨¡å¼
    for table in soup.find_all("table"):
        header_tr = None
        for tr in table.find_all("tr", recursive=True):
            if tr.find("th"):
                heads = [c.get_text(" ", strip=True) for c in tr.find_all(["th", "td"])]
                if any(("ç©ºä½" in h or "å‰©é¤˜" in h or "å¯å”®" in h) for h in heads):
                    header_tr = tr; break
        if not header_tr: continue
        heads = [c.get_text(" ", strip=True) for c in header_tr.find_all(["th", "td"])]
        try:
            idx_qty = next(i for i, h in enumerate(heads) if ("ç©ºä½" in h or "å‰©é¤˜" in h or "å¯å”®" in h))
        except StopIteration:
            continue
        try:
            idx_area = next(i for i, h in enumerate(heads) if ("ç¥¨å€" in h or h == "å€" or "åº§ä½å€" in h))
        except StopIteration:
            idx_area = 1 if len(heads) > 1 else 0

        tr = header_tr.find_next_sibling("tr")
        while tr and tr.name == "tr":
            tds = tr.find_all("td")
            if len(tds) > max(idx_qty, idx_area):
                area = tds[idx_area].get_text(" ", strip=True)
                qty_text = tds[idx_qty].get_text(" ", strip=True)
                if _RE_SOLDOUT.search(qty_text): soldout_hint = True
                m = re.search(r"(\d+)", qty_text)
                if m:
                    qty = int(m.group(1))
                    if qty > 0:
                        area = re.sub(r"\s+", "", area) or "æœªå‘½åå€"
                        sections[area] = sections.get(area, 0) + qty
                        total += qty
            tr = tr.find_next_sibling("tr")
        if total > 0: break

    # B) å€å¡Šå°±è¿‘æ¨¡å¼
    if total == 0:
        for node in soup.select("li, div, section, article, tr, p"):
            txt = node.get_text(" ", strip=True)
            if not txt: continue
            if _RE_SOLDOUT.search(txt): soldout_hint = True
            for m in _RE_QTY.finditer(txt):
                qty = int(m.group(2))
                if qty <= 0: continue
                area = _guess_area_from_text(txt[max(0, m.start()-40):m.end()+40])
                sections[area] = sections.get(area, 0) + qty
                total += qty

    return {"sections": sections, "total": total, "soldout": (total == 0 and soldout_hint), "soup": soup}

# ---------- 001 ç¥¨å€ï¼šURL/ID/åç¨± ----------
def _extract_ids_from_url(url: str) -> Dict[str, str]:
    q = dict((k.upper(), v) for k, v in parse_qsl(urlparse(url).query, keep_blank_values=True))
    return {"PERFORMANCE_ID": q.get("PERFORMANCE_ID",""), "PRODUCT_ID": q.get("PRODUCT_ID",""), "STRITEM": q.get("STRITEM","")}

def _collect_area_ids_from_html(html: str) -> Set[str]:
    ids: Set[str] = set()
    for m in re.finditer(r"PERFORMANCE_PRICE_AREA_ID\s*=\s*([A-Za-z0-9]+)", html, re.I):
        ids.add(m.group(1))
    for m in re.finditer(r"(UTK0201_001\.aspx[^\"'>)]+)", html):
        frag = m.group(1)
        mm = re.search(r"PERFORMANCE_PRICE_AREA_ID=([A-Za-z0-9]+)", frag, re.I)
        if mm: ids.add(mm.group(1))
    for m in re.finditer(r"data-?areaid\s*=\s*['\"]?([A-Za-z0-9]+)['\"]?", html, re.I):
        ids.add(m.group(1))
    return ids

def _build_001_base_url(base: str, perf_id: str, prod_id: str, stritem: str = "") -> str:
    p = urlparse(base)
    path = "/application/UTK02/UTK0201_001.aspx"
    q = f"PERFORMANCE_ID={quote(perf_id)}&PRODUCT_ID={quote(prod_id)}"
    if stritem: q += f"&strItem={quote(stritem)}"
    return f"{p.scheme}://{p.netloc}{path}?{q}"

def _build_001_url(base: str, perf_id: str, prod_id: str, area_id: str, stritem: str = "") -> str:
    p = urlparse(base)
    path = "/application/UTK02/UTK0201_001.aspx"
    q = f"PERFORMANCE_ID={quote(perf_id)}&PRODUCT_ID={quote(prod_id)}&PERFORMANCE_PRICE_AREA_ID={quote(area_id)}"
    if stritem: q += f"&strItem={quote(stritem)}"
    return f"{p.scheme}://{p.netloc}{path}?{q}"

def _extract_area_name_001(soup: BeautifulSoup, fallback: str) -> str:
    _strip_scripts(soup)
    name = _guess_area_from_text(soup.get_text(" ", strip=True))
    if name and name != "æœªå‘½åå€": return name
    og = soup.select_one('meta[property="og:title"]')
    if og and og.get("content"): return re.sub(r"\s+", "", og["content"])[:20]
    return fallback or "ç¥¨å€"

# ---------- æ´»å‹•ä¸»è¦–è¦ºåœ– ----------
def _find_image_in_soup(base_url: str, soup: BeautifulSoup) -> str:
    for prop in ("og:image", "og:image:url", "twitter:image"):
        og = soup.select_one(f'meta[property="{prop}"], meta[name="{prop}"]')
        if og and og.get("content"):
            return urljoin(base_url, og["content"])
    for img in soup.find_all("img"):
        cand = img.get("src") or img.get("data-src") or ""
        if not cand and img.get("srcset"):
            cand = img["srcset"].split(",")[0].split()[0]
        if cand and ("ActivityImage" in cand or "azureedge" in cand):
            return urljoin(base_url, cand)
    for u in re.findall(r'url\(([^)]+)\)', soup.decode()):
        u = u.strip('\'"')
        if "ActivityImage" in u or "azureedge" in u:
            return urljoin(base_url, u)
    return ""

def _resolve_activity_image(orders_url: str, html_000: str, soup_000: BeautifulSoup, sess: requests.Session) -> str:
    img = _find_image_in_soup(orders_url, soup_000)
    if img: return img
    m = _RE_ACTIVITY_IMG.search(html_000)
    if m: return m.group(0)
    m2 = _RE_ACTIVITY_INFO.search(html_000)
    if m2:
        details_url = m2.group(0)
        if not details_url.startswith("http"):
            details_url = urljoin(orders_url, details_url)
        try:
            r2 = sess.get(details_url, timeout=15); r2.raise_for_status()
            soup2 = BeautifulSoup(r2.text, "html.parser")
            _strip_scripts(soup2)
            img2 = _find_image_in_soup(details_url, soup2)
            if img2: return img2
            m3 = _RE_ACTIVITY_IMG.search(r2.text)
            if m3: return m3.group(0)
        except Exception as e:
            app.logger.warning(f"[image] details fetch fail: {details_url} {e}")
    return ""

# ---------- æ·±å…¥ 001 ç¥¨å€ ----------
def deep_parse_areas(orders_url: str, soup_000: BeautifulSoup, html_000: str, limit: int = 20) -> Tuple[Dict[str, int], int, bool]:
    ids = _collect_area_ids_from_html(html_000)
    info = _extract_ids_from_url(orders_url)
    perf, prod, stritem = info["PERFORMANCE_ID"], info["PRODUCT_ID"], info["STRITEM"]

    s = _req_session()
    s.headers.update({"Referer": orders_url})

    # 000 æ²’ id â†’ å…ˆæ‰“ 001 base çœ‹çœ‹
    if not ids:
        url_001_base = _build_001_base_url(orders_url, perf, prod, stritem)
        try:
            r0 = s.get(url_001_base, timeout=15); r0.raise_for_status()
            parsed0 = parse_ibon_orders_static(r0.text)
            if parsed0["total"] > 0:
                return parsed0["sections"], parsed0["total"], parsed0["soldout"]
            ids = _collect_area_ids_from_html(r0.text)
        except Exception as e:
            app.logger.warning(f"[deep] fetch 001 base fail: {e}")

    if not ids:
        return {}, 0, False

    ids = set(list(ids)[:limit])

    sections: Dict[str, int] = {}
    total = 0
    soldout_hint = False

    for area_id in ids:
        url_001 = _build_001_url(orders_url, perf, prod, area_id, stritem)
        try:
            r = s.get(url_001, timeout=15); r.raise_for_status()
        except Exception as e:
            app.logger.warning(f"[deep] fetch fail: {url_001} {e}")
            continue

        soup = BeautifulSoup(r.text, "html.parser")
        _strip_scripts(soup)
        text = soup.get_text(" ", strip=True)
        html = r.text

        if _RE_SOLDOUT.search(text) or _RE_SOLDOUT.search(html):
            soldout_hint = True

        area = _extract_area_name_001(soup, fallback=f"å€-{area_id[-4:]}")

        qsum = 0
        # æ–‡å­—
        for m in _RE_QTY.finditer(text):
            try: qsum += int(m.group(2))
            except: pass
        # HTML åŸæ–‡ï¼ˆå±¬æ€§/JSON å…§ï¼‰
        if qsum == 0:
            for m in _RE_QTY.finditer(html):
                try: qsum += int(m.group(2))
                except: pass
        # å¸¸è¦‹ JSON æ¬„ä½ä¿åº•
        if qsum == 0:
            for m in re.finditer(r'"(?:AvailableQty|Qty|Remain|Left|å‰©é¤˜|å¯å”®)"\s*:\s*(\d+)', html, re.I):
                qsum += int(m.group(1))

        if qsum > 0:
            key = re.sub(r"\s+", "", area) or f"å€-{area_id[-4:]}"
            sections[key] = sections.get(key, 0) + qsum
            total += qsum

    return sections, total, soldout_hint

# ---------- ä¸»æµç¨‹ ----------
def check_ibon(any_url: str) -> Tuple[bool, str, str, Dict[str, str]]:
    orders_url = resolve_ibon_orders_url(any_url)
    if not orders_url:
        return False, "æ‰¾ä¸åˆ° ibon ä¸‹å–®é ï¼ˆUTK0201ï¼‰ã€‚å¯èƒ½å°šæœªé–‹è³£æˆ–æŒ‰éˆ•æœªé¡¯ç¤ºã€‚", "NA", {}

    s = _req_session()
    r = s.get(orders_url, timeout=15); r.raise_for_status()

    parsed = parse_ibon_orders_static(r.text)
    meta = _extract_activity_meta(parsed["soup"])
    title = meta.get("title") or "æ´»å‹•è³‡è¨Š"
    venue = meta.get("venue")
    dt = meta.get("datetime")
    img = _resolve_activity_image(orders_url, r.text, parsed["soup"], s)

    # 000 æ²’æ•¸å­— â†’ è§£æ 001 ç¥¨å€ï¼ˆâ˜…â˜…â˜…æ³¨æ„ç¬¬ä¸‰å€‹åƒæ•¸è¦å‚³ r.textï¼‰
    if parsed["total"] == 0:
        sec2, tot2, sold2 = deep_parse_areas(orders_url, parsed["soup"], r.text, limit=20)
        if tot2 > 0:
            parsed["sections"], parsed["total"], parsed["soldout"] = sec2, tot2, False
        else:
            parsed["soldout"] = parsed["soldout"] or sold2

    prefix_lines = [f"ğŸ« {title}"]
    if venue: prefix_lines.append(f"åœ°é»ï¼š{venue}")
    if dt:    prefix_lines.append(f"æ—¥æœŸï¼š{dt}")
    prefix = "\n".join(prefix_lines) + "\n\n"

    if parsed["total"] > 0:
        parts = [f"{k}: {v} å¼µ" for k, v in sorted(parsed["sections"].items(), key=lambda kv: (-kv[1], kv[0]))]
        msg = prefix + "âœ… ç›£çœ‹çµæœï¼šç›®å‰å¯å”®\n" + "\n".join(parts) + f"\nåˆè¨ˆï¼š{parsed['total']} å¼µ\n{orders_url}"
        sig = hashlib.md5(("|" + "|".join(parts)).encode()).hexdigest()
        return True, msg, sig, {"image_url": img}
    if parsed.get("soldout"):
        msg = prefix + f"ç›®å‰é¡¯ç¤ºå®Œå”®/ç„¡ç¥¨\n{orders_url}"
        sig = hashlib.md5("soldout".encode()).hexdigest()
        return True, msg, sig, {"image_url": img}

    return False, prefix + "æš«æ™‚è®€ä¸åˆ°å‰©é¤˜æ•¸ï¼ˆå¯èƒ½ç‚ºå‹•æ…‹è¼‰å…¥ï¼‰ã€‚\n" + orders_url, "NA", {"image_url": img}

# ---------- Webhook ----------
def _get_target_id(src: dict) -> str:
    return src.get("userId") or src.get("groupId") or src.get("roomId") or ""

@app.post("/webhook")
def webhook():
    raw = request.get_data()
    if not _verify_line_signature(raw):
        app.logger.error("Invalid LINE signature")
        return "bad signature", 400

    body = request.get_json(silent=True) or {}
    events = body.get("events", [])

    for ev in events:
        etype = ev.get("type")
        if etype in ("follow", "join"):
            _line_reply(ev.get("replyToken"), HELP_TEXT); continue
        if etype != "message": continue

        msg = ev.get("message", {})
        if msg.get("type") != "text": continue

        text = (msg.get("text") or "").strip()
        reply_token = ev.get("replyToken")
        src = ev.get("source", {})
        low = text.lower()

        if low in ("/start", "start", "/help", "help", "ï¼Ÿ"):
            _line_reply(reply_token, HELP_TEXT); continue

        if low.startswith("/checkid"):
            parts = text.split()
            if len(parts) < 2:
                _line_reply(reply_token, "ç”¨æ³•ï¼š/checkid <ä»»å‹™ID>"); continue
            tid = parts[1].strip()
            docref = db.collection("watches").document(tid)
            doc = docref.get()
            if not doc.exists:
                _line_reply(reply_token, f"æ‰¾ä¸åˆ°ä»»å‹™ {tid}"); continue
            task = doc.to_dict()
            ok, msg_out, _sig, meta = check_ibon(task.get("url", ""))
            _line_reply_rich(reply_token, msg_out, (meta or {}).get("image_url")); continue

        if low.startswith("/check") or text == "æŸ¥è©¢":
            parts = text.split()
            url = None
            if len(parts) >= 2 and parts[1].startswith("http"):
                url = parts[1]
            if not url:
                target_id = _get_target_id(src)
                q = (db.collection("watches")
                     .where("targetId", "==", target_id)
                     .where("active", "==", True)
                     .order_by("createdAt", direction=firestore.Query.DESCENDING)
                     .limit(1))
                docs = list(q.stream())
                if docs: url = docs[0].to_dict().get("url")
            if not url:
                _line_reply(reply_token, "ç”¨æ³•ï¼š/check <ç¥¨åˆ¸ç¶²å€>\næˆ–å…ˆç”¨ /watch å»ºç«‹ä»»å‹™å¾Œè¼¸å…¥ã€ŒæŸ¥è©¢ã€"); continue
            ok, msg_out, _sig, meta = check_ibon(url)
            _line_reply_rich(reply_token, msg_out, (meta or {}).get("image_url")); continue

        if low.startswith("/watch"):
            parts = text.split()
            if len(parts) < 2:
                _line_reply(reply_token, "ç”¨æ³•ï¼š/watch <ç¥¨åˆ¸ç¶²å€> [ç§’]"); continue

            raw_url = parts[1]
            period = DEFAULT_PERIOD_SEC
            if len(parts) >= 3:
                try:
                    p = int(parts[2]); period = max(15, min(3600, p))
                except Exception: pass

            target_id = _get_target_id(src)
            target_type = "user" if src.get("userId") else ("group" if src.get("groupId") else "room")
            url_canon = canonicalize_ibon_url(raw_url)

            existing = None
            try:
                q0 = (db.collection("watches")
                      .where("targetId", "==", target_id)
                      .where("urlCanon", "==", url_canon)
                      .limit(1))
                docs0 = list(q0.stream())
                existing = docs0[0] if docs0 else None
            except Exception as e:
                app.logger.warning(f"/watch query by urlCanon failed, fallback scan: {e}")
            if not existing:
                q1 = (db.collection("watches")
                      .where("targetId", "==", target_id)
                      .order_by("createdAt", direction=firestore.Query.DESCENDING)
                      .limit(50))
                for d in q1.stream():
                    x = d.to_dict()
                    cand = x.get("urlCanon") or canonicalize_ibon_url(x.get("url", ""))
                    if cand == url_canon:
                        existing = d; break

            now = int(time.time())
            if existing:
                data = existing.to_dict()
                if data.get("active", True):
                    if int(data.get("periodSec", DEFAULT_PERIOD_SEC)) != period:
                        existing.reference.update({"periodSec": period, "nextCheckAt": now})
                        _line_reply(reply_token, f"æ­¤ç¶²å€å·²åœ¨ç›£çœ‹ âœ…\nä»»å‹™IDï¼š{existing.id}\nå·²æ›´æ–°ç‚ºæ¯ {period} ç§’æª¢æŸ¥\nURLï¼š{data.get('url')}")
                    else:
                        _line_reply(reply_token, f"æ­¤ç¶²å€å·²åœ¨ç›£çœ‹ âœ…\nä»»å‹™IDï¼š{existing.id}\næ¯ {data.get('periodSec', DEFAULT_PERIOD_SEC)} ç§’æª¢æŸ¥\nURLï¼š{data.get('url')}")
                else:
                    existing.reference.update({"active": True, "periodSec": period, "nextCheckAt": now})
                    _line_reply(reply_token, f"å·²é‡æ–°å•Ÿç”¨ âœ…\nä»»å‹™IDï¼š{existing.id}\næ¯ {period} ç§’æª¢æŸ¥\nURLï¼š{data.get('url')}")
                continue

            task_id = secrets.token_urlsafe(4)
            db.collection("watches").document(task_id).set({
                "url": raw_url,
                "urlCanon": url_canon,
                "targetType": target_type,
                "targetId": target_id,
                "periodSec": period,
                "nextCheckAt": now,
                "lastSig": None,
                "active": True,
                "createdAt": now,
            })
            _line_reply(reply_token, f"å·²é–‹å§‹ç›£çœ‹ âœ…\nä»»å‹™IDï¼š{task_id}\næ¯ {period} ç§’æª¢æŸ¥ä¸€æ¬¡\nURLï¼š{raw_url}")
            continue

        if low.startswith("/unwatch"):
            parts = text.split()
            if len(parts) < 2:
                _line_reply(reply_token, "ç”¨æ³•ï¼š/unwatch <ä»»å‹™ID>"); continue
            tid = parts[1]
            doc = db.collection("watches").document(tid)
            if doc.get().exists:
                doc.update({"active": False})
                _line_reply(reply_token, f"ä»»å‹™ {tid} å·²åœç”¨")
            else:
                _line_reply(reply_token, f"æ‰¾ä¸åˆ°ä»»å‹™ {tid}")
            continue

        if low.startswith("/list"):
            mode = "on"
            if len(text.split()) >= 2:
                opt = text.split()[1].lower()
                if opt in ("all", "-a", "--all"): mode = "all"
                elif opt in ("off", "--off"):      mode = "off"

            target_id = _get_target_id(src)
            q = db.collection("watches").where("targetId", "==", target_id)
            if mode == "on":  q = q.where("active", "==", True)
            if mode == "off": q = q.where("active", "==", False)
            q = q.order_by("createdAt", direction=firestore.Query.DESCENDING).limit(20)

            docs = list(q.stream())
            if not docs:
                _line_reply(reply_token, "ç›®å‰æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„ä»»å‹™" + ("" if mode=="on" else f"ï¼ˆ{mode}ï¼‰"))
            else:
                lines = []
                for d in docs:
                    x = d.to_dict()
                    flag = "å•Ÿç”¨" if x.get("active") else "åœç”¨"
                    if mode == "on" and not x.get("active"): continue
                    lines.append(f"{d.id}ï½œ{flag}ï½œ{x.get('periodSec', 60)}s\n{x.get('url')}")
                _line_reply(reply_token, ("ä½ çš„ä»»å‹™ï¼š" if mode=="on" else ("ä½ çš„ä»»å‹™ï¼ˆå…¨éƒ¨ï¼‰ï¼š" if mode=="all" else "ä½ çš„ä»»å‹™ï¼ˆåœç”¨ï¼‰ï¼š")) + "\n" + "\n\n".join(lines))
            continue

        _line_reply(reply_token, HELP_TEXT)

    return "OK"

# ---------- æ’ç¨‹ ----------
def enqueue_tick_runs(delays=(0, 15, 30, 45)) -> int:
    try:
        from google.cloud import tasks_v2
        from google.protobuf import timestamp_pb2
    except Exception as e:
        app.logger.warning(f"[fanout] cloud-tasks lib missing, run once. {e}")
        return 0

    if not (PROJECT_ID and TASKS_QUEUE and TASKS_LOCATION and TASKS_SERVICE_ACCOUNT and TASKS_TARGET_URL):
        app.logger.warning("[fanout] env incomplete; run once instead.")
        return 0

    client = tasks_v2.CloudTasksClient()
    parent = client.queue_path(PROJECT_ID, TASKS_LOCATION, TASKS_QUEUE)
    created = 0

    for d in delays:
        ts = timestamp_pb2.Timestamp(); ts.FromSeconds(int(time.time()) + int(d))
        task = {
            "http_request": {
                "http_method": tasks_v2.HttpMethod.GET,
                "url": f"{TASKS_TARGET_URL}/cron/tick?mode=run",
                "headers": {"User-Agent": "Cloud-Tasks", "X-From-Tasks": "1"},
                "oidc_token": {"service_account_email": TASKS_SERVICE_ACCOUNT, "audience": TASKS_TARGET_URL},
            },
            "schedule_time": ts
        }
        try:
            client.create_task(request={"parent": parent, "task": task}); created += 1
        except Exception as e:
            app.logger.exception(f"[fanout] create_task failed (delay={d}): {e}")

    return created

def do_tick():
    now = int(time.time())
    q = (db.collection("watches")
         .where("active", "==", True)
         .where("nextCheckAt", "<=", now)
         .limit(MAX_TASKS_PER_TICK))
    try:
        docs = list(q.stream())
    except Exception as e:
        app.logger.exception(f"[tick] Firestore query failed: {e}")
        return jsonify({"ok": False, "stage": "query", "error": str(e)}), 200

    processed, errors = 0, []
    for d in docs:
        task = d.to_dict()
        try:
            ok, msg, sig, meta = check_ibon(task["url"])
            if ok:
                should_push = (ALWAYS_NOTIFY or sig != task.get("lastSig"))
                if should_push:
                    _line_push_rich(task["targetId"], msg, (meta or {}).get("image_url"))
                    d.reference.update({"lastSig": sig})
        except Exception as e:
            app.logger.exception(f"[tick] task {d.id} failed: {e}")
            errors.append(f"{d.id}:{type(e).__name__}")
        finally:
            period = int(task.get("periodSec", DEFAULT_PERIOD_SEC))
            d.reference.update({"nextCheckAt": now + max(15, period)})
            processed += 1

    return jsonify({"ok": True, "processed": processed, "due": len(docs), "errors": errors, "ts": now}), 200

@app.get("/cron/tick")
def cron_tick():
    # Scheduler ç›´å‘¼ï¼šæœªå¸¶ mode=run æ™‚ï¼Œå¯é¸æ“‡æ‰‡å‡º
    if request.args.get("mode") == "run" or request.headers.get("X-From-Tasks") == "1":
        return do_tick()
    if TICK_FANOUT:
        n = enqueue_tick_runs((0, 15, 30, 45))
        if n > 0: return jsonify({"ok": True, "fanout": n}), 200
    return do_tick()

@app.get("/healthz")
def healthz():
    return "ok", 200

# æ–¹ä¾¿è‡ªæŸ¥
@app.get("/diag")
def diag():
    url = request.args.get("url", "")
    if not url: return jsonify({"error":"missing url"}), 400
    ok, msg, sig, meta = check_ibon(url)
    return jsonify({"ok": ok, "sig": sig, "image": meta.get("image_url") if meta else None, "msg": msg})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", "8080")))