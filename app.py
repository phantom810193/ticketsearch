# -*- coding: utf-8 -*-
import os
import re
import json
import time
import uuid
import hashlib
import logging
import traceback
import requests
from datetime import datetime, timezone, timedelta
from typing import Dict, Tuple, Optional, Any, List
from urllib.parse import urlparse, urlunparse, parse_qs, urlencode, urljoin
from flask import send_from_directory
from flask import jsonify, Flask, request, abort

IBON_API = "https://ticketapi.ibon.com.tw/api/ActivityInfo/GetIndexData"
IBON_BASE = "https://ticket.ibon.com.tw/"

# ç°¡å–®å¿«å–ï¼ˆ5 åˆ†é˜ï¼‰
_cache = {"ts": 0, "data": []}
_CACHE_TTL = 300  # ç§’

_CONCERT_WORDS = ("æ¼”å”±æœƒ", "æ¼”å ´æœƒ", "éŸ³æ¨‚æœƒ", "æ¼”å”±", "æ¼”å‡º", "LIVE", "Live")

def _looks_like_concert(title: str) -> bool:
    t = title or ""
    return any(w.lower() in t.lower() for w in _CONCERT_WORDS)

def _normalize_item(row):
    """
    å°‡ ibon API çš„æ´»å‹•é …ç›®è½‰ç‚º {title,url,image}
    å…¼å®¹ä¸åŒæ¬„ä½åç¨±ã€‚
    """
    title = row.get("Title") or row.get("ActivityTitle") or row.get("Name") or "æ´»å‹•"
    img = (row.get("ImgUrl") or row.get("ImageUrl") or row.get("Image") or "").strip() or None

    # å¯èƒ½ç›´æ¥çµ¦ Urlï¼Œä¹Ÿå¯èƒ½åªæœ‰ ActivityId/Id
    url = (row.get("Url") or row.get("LinkUrl") or "").strip()
    if not url:
        act_id = row.get("ActivityId") or row.get("Id") or row.get("ID")
        if act_id:
            url = urljoin(IBON_BASE, f"/ActivityInfo/Details/{act_id}")

    # çµ±ä¸€ç‚ºçµ•å°ç¶²å€
    if url and not url.lower().startswith("http"):
        url = urljoin(IBON_BASE, url)

    # æœ‰äº›åœ–ç‰‡çµ¦ç›¸å°è·¯å¾‘
    if img and not img.lower().startswith("http"):
        img = urljoin(IBON_BASE, img)

    return {"title": title.strip(), "url": url, "image": img}

def fetch_ibon_list_via_api(limit=10, keyword=None, only_concert=False):
    """
    ç›´æ¥æ‰“ ibon å®˜æ–¹ API æŠ½å–æ´»å‹•æ¸…å–®ã€‚
    å›å‚³ï¼š[{title, url, image}, ...]
    """
    global _cache
    now = time.time()
    if now - _cache["ts"] < _CACHE_TTL and _cache["data"]:
        rows = _cache["data"]
    else:
        headers = {
            "User-Agent": "Mozilla/5.0",
            "Accept": "application/json, text/plain, */*",
            "Origin": "https://ticket.ibon.com.tw",
            "Referer": "https://ticket.ibon.com.tw/Index/entertainment",
        }
        # å¤šæ•¸æƒ…æ³ç”¨ POSTï¼›è‹¥ GET ä¹Ÿèƒ½å›è³‡æ–™ï¼Œå¯æ”¹æˆ requests.get(...)
        try:
            resp = requests.post(IBON_API, headers=headers, timeout=12, json={})
            resp.raise_for_status()
            data = resp.json()
        except Exception as e:
            logging.getLogger().error(f"[ibon api] request failed: {e}")
            return []

        # è§£æå¯èƒ½çš„åŒ…è£å±¤
        # å¸¸è¦‹å±¤ç´šï¼š{ "Data": { <å¤šå€‹å€å¡Š> } } æˆ– { "data": [...] } ç­‰
        blobs = []
        root = data.get("Data") or data.get("data") or data
        if isinstance(root, dict):
            # æ”¶é›†æ‰€æœ‰é™£åˆ—æ¬„ä½ï¼ˆåŒ…å« "ActivityList", "Items" ç­‰ï¼‰
            for v in root.values():
                if isinstance(v, list):
                    blobs.extend(v)
                elif isinstance(v, dict) and isinstance(v.get("ActivityList"), list):
                    blobs.extend(v["ActivityList"])
        elif isinstance(root, list):
            blobs = root

        # æœ€å¾Œå†ä¿åº•ï¼šå¦‚æœé‚„æ˜¯ dictï¼Œæ‰¾å‡ºæ‰€æœ‰å…§å±¤ list
        if not blobs and isinstance(data, dict):
            for v in data.values():
                if isinstance(v, list):
                    blobs.extend(v)

        rows = []
        for r in blobs:
            try:
                item = _normalize_item(r)
                if not item.get("url"):
                    continue
                rows.append(item)
            except Exception:
                continue

        _cache = {"ts": now, "data": rows}

    # é—œéµå­—/åªè¦æ¼”å”±æœƒçš„éæ¿¾
    out = []
    kw = (keyword or "").strip()
    for it in rows:
        if kw and kw not in it["title"]:
            continue
        if only_concert and not _looks_like_concert(it["title"]):
            continue
        out.append(it)
        if len(out) >= max(1, int(limit)):
            break
    return out

# --------- LINE SDKï¼ˆå¯é¸ï¼‰---------
HAS_LINE = True
try:
    from linebot import LineBotApi, WebhookHandler
    from linebot.exceptions import InvalidSignatureError
    from linebot.models import (
        MessageEvent, TextMessage, TextSendMessage, ImageSendMessage,
        FollowEvent, JoinEvent,
    )
except Exception as e:
    HAS_LINE = False
    LineBotApi = WebhookHandler = InvalidSignatureError = None
    MessageEvent = TextMessage = TextSendMessage = ImageSendMessage = None
    logging.warning(f"[init] line-bot-sdk not available: {e}")

# --------- Firestoreï¼ˆå¯å¤±æ•—ä¸è‡´å‘½ï¼‰---------
from google.cloud import firestore

# HTML è§£æ
from bs4 import BeautifulSoup

# ===== Flask & CORS =====
app = Flask(__name__)

# å»ºè­°ï¼šç™½åå–®ï¼ˆå¯å¤šå€‹ç¶²åŸŸï¼‰
# 1) ä¸€å®šè¦åŒ…å« LIFF çš„ç¶²åŸŸï¼š https://liff.line.me
# 2) å†åŠ å…¥ä½  Cloud Run çš„å®Œæ•´ç¶²å€ï¼ˆè«‹æ”¹æˆä½ çš„å¯¦éš›ç¶²å€ï¼‰
ALLOWED_ORIGINS = [
    "https://liff.line.me",
    "https://ticketsearch-419460755270.asia-east1.run.app",  # â† æ›æˆä½ çš„
]

try:
    from flask_cors import CORS  # type: ignore
    # åªé–‹ /liff/* è·¯å¾‘ï¼ˆ/liff/activitiesã€/liff/ ç­‰ï¼‰
    CORS(
        app,
        resources={
            r"/liff/*": {
                "origins": ALLOWED_ORIGINS,
                # å¯è¦–éœ€è¦è£œä¸Šå…è¨±çš„æ–¹æ³•/æ¨™é ­ï¼ˆé è¨­å°±å¤ ç”¨ï¼‰
                # "methods": ["GET", "OPTIONS"],
                # "allow_headers": ["Content-Type"],
            }
        },
        supports_credentials=True,  # ä½ å‰ç«¯ fetch æœ‰å¸¶ credentials æ™‚éœ€è¦
    )
except Exception as e:
    app.logger.warning(f"flask-cors not available: {e}")
    logging.basicConfig(level=os.getenv("LOG_LEVEL", "INFO"))
    app.logger.setLevel(logging.INFO)

# ======== ç’°å¢ƒè®Šæ•¸ ========
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN", "")
LINE_CHANNEL_SECRET = os.getenv("LINE_CHANNEL_SECRET", "")
DEFAULT_PERIOD_SEC = int(os.getenv("DEFAULT_PERIOD_SEC", "60"))
ALWAYS_NOTIFY = os.getenv("ALWAYS_NOTIFY", "0") == "1"
FOLLOW_AREAS_PER_CHECK = int(os.getenv("FOLLOW_AREAS_PER_CHECK", "0"))  # é è¨­ 0ï¼šä¸è¿½ç¥¨å€ç¬¬äºŒæ­¥é 

# å¯é¸ï¼šæ‰‹å‹•è¦†è“‹å®£å‚³åœ–æˆ– Details é€£çµï¼ˆé€šå¸¸ä¸éœ€è¦ï¼‰
PROMO_IMAGE_MAP: Dict[str, str] = {}
PROMO_DETAILS_MAP: Dict[str, str] = {}
try:
    PROMO_IMAGE_MAP = json.loads(os.getenv("PROMO_IMAGE_MAP", "{}"))
except Exception:
    PROMO_IMAGE_MAP = {}
try:
    PROMO_DETAILS_MAP = json.loads(os.getenv("PROMO_DETAILS_MAP", "{}"))
except Exception:
    PROMO_DETAILS_MAP = {}

if not LINE_CHANNEL_ACCESS_TOKEN or not LINE_CHANNEL_SECRET:
    app.logger.warning("LINE env not set: LINE_CHANNEL_ACCESS_TOKEN / LINE_CHANNEL_SECRET")

line_bot_api = LineBotApi(LINE_CHANNEL_ACCESS_TOKEN) if (HAS_LINE and LINE_CHANNEL_ACCESS_TOKEN) else None
handler = WebhookHandler(LINE_CHANNEL_SECRET) if (HAS_LINE and LINE_CHANNEL_SECRET) else None

MAX_PER_TICK = int(os.getenv("MAX_PER_TICK", "6"))
TICK_SOFT_DEADLINE_SEC = int(os.getenv("TICK_SOFT_DEADLINE_SEC", "50"))

# Firestore client
try:
    fs_client = firestore.Client()
    FS_OK = True
except Exception as e:
    app.logger.warning(f"Firestore init failed: {e}")
    fs_client = None
    FS_OK = False

COL = "watchers"

UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/122.0 Safari/537.36"
)

_RE_DATE = re.compile(r"(\d{4}/\d{2}/\d{2})\s+(\d{2}:\d{2})")
_RE_AREA_TAG = re.compile(r"<area\b[^>]*>", re.I)

LOGO = "https://ticketimg2.azureedge.net/logo.png"
TICKET_API = "https://ticketapi.ibon.com.tw/api/ActivityInfo/GetGameInfoList"

# ================= å°å·¥å…· =================
def soup_parse(html: str) -> BeautifulSoup:
    try:
        return BeautifulSoup(html, "lxml")
    except Exception:
        return BeautifulSoup(html, "html.parser")

def hash_state(sections: Dict[str, int], selling: List[str]) -> str:
    """å°‡ã€æœ‰æ•¸å­—å€ã€èˆ‡ã€ç†±è³£ä¸­å€ã€ä¸€èµ·åšç°½ç« ï¼Œé¿å…åªçœ‹æ•¸å­—æ¼é€šçŸ¥ã€‚"""
    items = sorted((k, int(v)) for k, v in sections.items())
    hot = sorted(selling)
    raw = json.dumps({"num": items, "hot": hot}, ensure_ascii=False, separators=(",", ":"))
    return hashlib.md5(raw.encode("utf-8")).hexdigest()

def canonicalize_url(u: str) -> str:
    p = urlparse(u.strip())
    q = parse_qs(p.query, keep_blank_values=True)
    q_sorted = []
    for k in sorted(q.keys()):
        for v in q[k]:
            q_sorted.append((k, v))
    new_q = urlencode(q_sorted, doseq=True)
    return urlunparse((p.scheme, p.netloc, p.path, "", new_q, ""))

def send_text(to_id: str, text: str):
    if not line_bot_api:
        app.logger.info(f"[dry-run] send_text to {to_id}: {text}")
        return
    try:
        line_bot_api.push_message(to_id, TextSendMessage(text=text))
    except Exception as e:
        app.logger.error(f"[LINE] push text failed: {e}")

def send_image(to_id: str, img_url: str):
    if not line_bot_api:
        app.logger.info(f"[dry-run] send_image to {to_id}: {img_url}")
        return
    try:
        line_bot_api.push_message(
            to_id,
            ImageSendMessage(original_content_url=img_url, preview_image_url=img_url)
        )
    except Exception as e:
        app.logger.error(f"[LINE] push image failed: {e}")

def sess_default() -> requests.Session:
    s = requests.Session()
    s.headers.update({
        "User-Agent": UA,
        "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.6",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Connection": "close",
    })
    return s

def _url_ok(u: str) -> bool:
    if not u or not u.startswith("http"):
        return False
    try:
        r = requests.head(u, timeout=6, allow_redirects=True)
        if r.status_code in (403, 405):  # æŸäº› CDN ç¦ HEAD
            r = requests.get(u, stream=True, timeout=8)
        return 200 <= r.status_code < 400
    except Exception:
        return False

def _first_http_url(s: str) -> Optional[str]:
    m = re.search(r'https?://[^\s"\'<>]+', str(s))
    return m.group(0) if m else None

def find_activity_image_any(s: str) -> Optional[str]:
    m = re.search(r"https?://[^\"'<>]+/image/ActivityImage/[^\s\"'<>]+\.(?:jpg|jpeg|png)", s, flags=re.I)
    if m: return m.group(0)
    m = re.search(r"https?://ticketimg2\.azureedge\.net/[^\s\"'<>]+\.(?:jpg|jpeg|png)", s, flags=re.I)
    if m: return m.group(0)
    m = re.search(r"https?://img\.ibon\.com\.tw/[^\s\"'<>]+\.(?:jpg|jpeg|png)", s, flags=re.I)
    return m.group(0) if m else None

def find_details_url_candidates_from_html(html: str, base: str) -> List[str]:
    soup = soup_parse(html)
    urls: set[str] = set()
    for a in soup.select('a[href*="ActivityInfo/Details"]'):
        href = (a.get("href") or "").strip()
        if href:
            urls.add(urljoin(base, href))
    for m in re.finditer(r"(?:https?://ticket\.ibon\.com\.tw)?/ActivityInfo/Details/\d+", html):
        urls.add(urljoin("https://ticket.ibon.com.tw", m.group(0)))
    return list(urls)

# ---------- æ´»å‹•è³‡è¨Šèˆ‡åœ–ç‰‡ï¼ˆAPI/Detailsï¼‰ ----------
def _deep_pick_activity_info(data: Any) -> Dict[str, str]:
    out: Dict[str, Optional[str]] = {"title": None, "place": None, "dt": None, "poster": None}
    def walk(x):
        if isinstance(x, dict):
            for k, v in x.items():
                kl = str(k).lower()
                if not out["title"] and any(t in kl for t in ("activityname","gamename","title","actname","activity_title","name")):
                    if isinstance(v, str) and v.strip(): out["title"] = v.strip()
                if not out["place"] and any(t in kl for t in ("placename","venue","place","site","location")):
                    if isinstance(v, str) and v.strip(): out["place"] = v.strip()
                if not out["dt"] and any(t in kl for t in ("starttime","startdatetime","gamedatetime","gamedate","begindatetime","datetime")):
                    s = str(v)
                    m = re.search(r"(\d{4})[-/](\d{1,2})[-/](\d{1,2})[\sT]+(\d{1,2}):(\d{2})", s)
                    if m:
                        out["dt"] = f"{int(m.group(1))}/{int(m.group(2)):02d}/{int(m.group(3)):02d} {int(m.group(4)):02d}:{m.group(5)}"
                if not out["poster"] and ("image" in kl or "poster" in kl):
                    url = _first_http_url(v) if isinstance(v, str) else None
                    if url: out["poster"] = url
            for v in x.values(): walk(v)
        elif isinstance(x, list):
            for it in x: walk(it)
    walk(data)
    return {k: v for k, v in out.items() if v}

def fetch_game_info_from_api(perf_id: Optional[str], product_id: Optional[str], referer_url: str, sess: requests.Session) -> Dict[str, str]:
    headers = {
        "Origin": "https://orders.ibon.com.tw",
        "Referer": referer_url,
        "User-Agent": UA,
        "Accept": "application/json, text/plain, */*",
        "Content-Type": "application/json;charset=UTF-8",
        "X-Requested-With": "XMLHttpRequest",
        "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.6",
    }
    tries: List[Tuple[str, Dict[str, Optional[str]]]] = []
    if perf_id or product_id:
        tries += [
            ("GET",  {"Performance_ID": perf_id, "Product_ID": product_id}),
            ("GET",  {"PerformanceId": perf_id,  "ProductId":  product_id}),
            ("GET",  {"PERFORMANCE_ID": perf_id,"PRODUCT_ID": product_id}),
            ("POST", {"Performance_ID": perf_id, "Product_ID": product_id}),
            ("POST", {"PerformanceId": perf_id,  "ProductId":  product_id}),
        ]
    tries.append(("GET", {}))  # ç„¡åƒæ•¸ä¹Ÿè©¦

    picked: Dict[str, str] = {}
    for method, params in tries:
        try:
            if method == "GET":
                r = sess.get(TICKET_API, params={k: v for k, v in (params or {}).items() if v}, headers=headers, timeout=12)
            else:
                r = sess.post(TICKET_API, json={k: v for k, v in (params or {}).items() if v}, headers=headers, timeout=12)
            if r.status_code != 200:
                continue

            data = r.json()
            s = json.dumps(data, ensure_ascii=False)

            info = _deep_pick_activity_info(data)

            m = re.search(r'https?://ticket\.ibon\.com\.tw/ActivityInfo/Details/(\d+)', s)
            if m:
                info["details"] = m.group(0)
            else:
                m = (re.search(r'"ActivityInfoId"\s*:\s*(\d+)', s) or
                     re.search(r'"ActivityId"\s*:\s*(\d+)', s) or
                     re.search(r'"Id"\s*:\s*(\d+)', s))
                if m:
                    info["details"] = f"https://ticket.ibon.com.tw/ActivityInfo/Details/{m.group(1)}"

            if not info.get("poster"):
                promo = find_activity_image_any(s)
                if promo:
                    info["poster"] = promo

            def match_obj(obj):
                text = json.dumps(obj, ensure_ascii=False)
                ok = True
                if perf_id: ok = ok and (perf_id in text)
                if product_id: ok = ok and (product_id in text)
                return ok
            if isinstance(data, list):
                for it in data:
                    if match_obj(it): info.update(_deep_pick_activity_info(it)); break
            elif isinstance(data, dict):
                for v in data.values():
                    if isinstance(v, list):
                        for it in v:
                            if match_obj(it): info.update(_deep_pick_activity_info(it)); break

            if info:
                picked = info
                break

        except Exception as e:
            app.logger.info(f"[api] fetch fail ({method} {params}): {e}")
            continue

    return picked

def fetch_from_ticket_details(details_url: str, sess: requests.Session) -> Dict[str, str]:
    out: Dict[str, str] = {}
    try:
        r = sess.get(details_url, timeout=12)
        if r.status_code != 200:
            return out
        html = r.text
        soup = soup_parse(html)

        for sel in [
            'meta[property="og:image:secure_url"]',
            'meta[property="og:image"]',
            'meta[name="twitter:image"]',
        ]:
            m = soup.select_one(sel)
            if m and m.get("content"):
                out["poster"] = urljoin(details_url, m["content"].strip())
                break
        if not out.get("poster"):
            for img in soup.find_all("img"):
                src = (img.get("src") or "").strip()
                if src and any(k in src.lower() for k in ("activityimage","azureedge","banner","cover","adimage")):
                    out["poster"] = urljoin(details_url, src); break

        h1 = soup.select_one("h1")
        if h1:
            t = h1.get_text(" ", strip=True)
            if t: out["title"] = t

        tx = soup.get_text(" ", strip=True)
        m = re.search(r'(\d{4}/\d{2}/\d{2})\s*(?:\([\u4e00-\u9fff]\))?\s*(\d{2}:\d{2})', tx)
        if m: out["dt"] = f"{m.group(1)} {m.group(2)}"

    except Exception as e:
        app.logger.info(f"[details] fetch fail: {e}")
    return out

# ---- åœ–ç‰‡ï¼ˆå®£å‚³åœ– + åº§ä½åœ–ï¼‰----
def pick_event_images_from_000(html: str, base_url: str) -> Tuple[str, Optional[str]]:
    poster = LOGO
    seatmap = None
    try:
        soup = soup_parse(html)
        for img in soup.find_all("img"):
            src = (img.get("src") or "").strip()
            if src and "static_bigmap" in src.lower():
                seatmap = urljoin(base_url, src); break
        if not seatmap:
            m = re.search(r'https?://[^\s"\'<>]+static_bigmap[^\s"\'<>]+?\.(?:jpg|jpeg|png)', html, flags=re.I)
            if m: seatmap = m.group(0)

        promo = find_activity_image_any(html)
        if promo:
            poster = promo
        else:
            for sel in ['meta[property="og:image"]', 'meta[name="twitter:image"]']:
                m = soup.select_one(sel)
                if m and m.get("content"):
                    poster = urljoin(base_url, m["content"]); break
            if poster == LOGO:
                for img in soup.find_all("img"):
                    src = (img.get("src") or "").strip()
                    if src and any(k in src.lower() for k in ("activityimage","azureedge","adimage")):
                        poster = urljoin(base_url, src); break
    except Exception as e:
        app.logger.warning(f"[image] pick failed: {e}")
    return poster, seatmap

def extract_title_place_from_html(html: str) -> tuple[Optional[str], Optional[str], Optional[str]]:
    soup = soup_parse(html)

    title: Optional[str] = None
    place: Optional[str] = None
    dt_text: Optional[str] = None

    # å–æ¨™é¡Œ / å ´åœ°
    for gt in soup.select('.grid-title'):
        lab = gt.get_text(" ", strip=True)
        sib = gt.find_next_sibling()
        if not sib:
            continue
        content = sib.get_text(" ", strip=True)
        if not content:
            continue

        if any(k in lab for k in ("æ´»å‹•åç¨±", "æ¼”å‡ºåç¨±", "ç¯€ç›®åç¨±", "å ´æ¬¡åç¨±")) and not title:
            title = content

        if any(k in lab for k in ("æ´»å‹•åœ°é»", "åœ°é»", "å ´åœ°")) and not place:
            place = re.sub(r"\s+", " ", content).strip()

    if not title:
        m = soup.select_one('[id$="_NAME"]')
        if m:
            t = m.get_text(" ", strip=True)
            if t: title = t

    if not title:
        h1 = soup.select_one("h1")
        if h1:
            t = h1.get_text(" ", strip=True)
            if len(t) >= 6: title = t

    if not title:
        mt = soup.select_one('meta[property="og:title"]')
        if mt and mt.get("content"):
            title = mt["content"].strip()

    m = _RE_DATE.search(html)
    if m:
        dt_text = f"{m.group(1)} {m.group(2)}"

    return title, place, dt_text

# ============= ç¥¨å€èˆ‡ live.map è§£æ =============
def extract_area_meta_from_000(html: str) -> Tuple[Dict[str, str], Dict[str, str], Dict[str, int], Dict[str, int]]:
    """
    å¾ UTK0201_000 æŠ½å‡ºï¼š
      - name_map   : å€ä»£ç¢¼ -> ä¸­æ–‡åç¨±
      - status_map : å€ä»£ç¢¼ -> ç‹€æ…‹å­—ï¼ˆç†±è³£ä¸­/å·²å”®å®Œ/â€¦ï¼‰
      - qty_map    : å€ä»£ç¢¼ -> è¡¨æ ¼ã€Œç©ºä½ã€æ¬„çš„æ•¸å­—ï¼ˆè‹¥æœ‰ï¼‰
      - order_map  : å€ä»£ç¢¼ -> é¡¯ç¤ºé †åºï¼ˆjsonData SORTï¼›å¦å‰‡è¡¨æ ¼åˆ—é †ï¼‰
    """
    name_map: Dict[str, str] = {}
    status_map: Dict[str, str] = {}
    qty_map: Dict[str, int] = {}
    order_map: Dict[str, int] = {}

    soup = soup_parse(html)

    # (a) script jsonData
    for sc in soup.find_all("script"):
        s = sc.string or sc.text or ""
        m = re.search(r"jsonData\s*=\s*'(\[.*?\])'", s, flags=re.S)
        if not m:
            continue
        try:
            arr = json.loads(m.group(1))
            for it in arr:
                code = (it.get("PERFORMANCE_PRICE_AREA_ID") or "").strip()
                name = (it.get("NAME") or "").strip()
                amt  = (it.get("AMOUNT") or "").strip()
                srt  = it.get("SORT")
                if code and name:
                    name_map.setdefault(code, re.sub(r"\s+", "", name))
                if code and amt:
                    status_map.setdefault(code, amt)
                    nums = [int(x) for x in re.findall(r"\d+", amt) if int(x) < 1000]
                    if nums:
                        qty_map.setdefault(code, nums[-1])
                if code and isinstance(srt, int):
                    order_map.setdefault(code, srt)
        except Exception:
            pass

    # (b) è¡¨æ ¼åˆ—
    row_idx = 0
    for a in soup.select('a[href*="PERFORMANCE_PRICE_AREA_ID="]'):
        href = a.get("href", "")
        m = re.search(r'PERFORMANCE_PRICE_AREA_ID=([A-Za-z0-9]+)', href)
        if not m:
            continue
        code = m.group(1)
        row_idx += 1
        order_map.setdefault(code, 10000 + row_idx)  # æ²’ SORT å°±ç”¨è¡¨æ ¼åº

        tr = a.find_parent("tr")
        if not tr:
            continue

        tds = [td.get_text(" ", strip=True) for td in tr.find_all("td")]
        if tds:
            if code not in name_map:
                cand = None
                for t in tds:
                    if re.search(r"(æ¨“|å€|åŒ…å»‚)", t):
                        cand = t; break
                if not cand:
                    cand = tds[0]
                name_map[code] = re.sub(r"\s+", "", cand)

            status_cell = ""
            for t in reversed(tds):
                if ("å·²å”®å®Œ" in t) or ("ç†±è³£" in t) or re.search(r"\b\d{1,3}\b", t):
                    status_cell = t
                    break
            if status_cell:
                if code not in status_map:
                    if "å·²å”®å®Œ" in status_cell:
                        status_map[code] = "å·²å”®å®Œ"
                    elif "ç†±è³£" in status_cell:
                        status_map[code] = "ç†±è³£ä¸­"
                nums = [int(x) for x in re.findall(r"\d+", status_cell) if int(x) < 1000]
                if nums and code not in qty_map:
                    qty_map[code] = nums[-1]

    return name_map, status_map, qty_map, order_map

def _parse_livemap_text(txt: str) -> Tuple[Dict[str, int], int]:
    """åªèª data-left / é—œéµå­—ã€å‰©é¤˜|å°šé¤˜|å¯å”®|å¯è³¼ã€æˆ–ã€(\d+) å¼µã€ï¼›åŒä¸€å€å–æœ€å¤§å€¼ã€‚"""
    sections: Dict[str, int] = {}
    for tag in _RE_AREA_TAG.findall(txt):
        code = None
        m = re.search(
            r"javascript:Send\([^)]*'(?P<perf>B0[0-9A-Z]{6,10})'\s*,\s*'(?P<area>B0[0-9A-Z]{6,10})'",
            tag, re.I
        )
        if m: code = m.group("area")
        if not code:
            m = re.search(r'(?:data-(?:area|area-id|price-area-id))=["\'](B0[0-9A-Z]{6,10})["\']', tag, re.I)
            if m: code = m.group(1)
        if not code:
            continue

        qty = None
        m = re.search(r'\bdata-(?:left|remain|qty|count)=["\']?(\d{1,3})["\']?', tag, re.I)
        if m:
            qty = int(m.group(1))

        if qty is None:
            text = ""
            m = re.search(r'title="([^"]*)"', tag, re.I)
            if m: text = m.group(1)
            if not text:
                m = re.search(r'(?:alt|aria-label)=["\']([^"\']*)["\']', tag, re.I)
                if m: text = m.group(1)
            if text:
                m = re.search(r'(?:å‰©é¤˜|å°šé¤˜|å¯å”®|å¯è³¼)[^\d]{0,6}(\d{1,3})', text)
                if not m:
                    m = re.search(r'(\d{1,3})\s*å¼µ', text)
                if m:
                    qty = int(m.group(1))

        if not qty or qty <= 0 or qty > 500:
            continue

        prev = sections.get(code)
        if prev is None or qty > prev:
            sections[code] = qty

    total = sum(sections.values())
    return sections, total

def try_fetch_livemap_by_perf(perf_id: str, sess: requests.Session, html: Optional[str] = None) -> Tuple[Dict[str, int], int]:
    if not perf_id:
        return {}, 0
    bases = [f"https://qwareticket-asysimg.azureedge.net/QWARE_TICKET/images/Temp/{perf_id}/"]
    if html:
        poster, seatmap = pick_event_images_from_000(html, "https://orders.ibon.com.tw/")
        if seatmap:
            m = re.match(r'(https?://.*/images/[^/]+/)', seatmap)
            if m: bases.insert(0, m.group(1))
    prefixes = ["", "1_", "2_", "3_", "01_", "02_", "03_"]
    tried = set()
    for base in bases:
        for pref in prefixes:
            url = f"{base}{pref}{perf_id}_live.map"
            if url in tried: continue
            tried.add(url)
            try:
                app.logger.info(f"[livemap] try {url}")
                r = sess.get(url, timeout=12)
                if r.status_code == 200 and "<area" in r.text:
                    app.logger.info(f"[livemap] hit {url}")
                    return _parse_livemap_text(r.text)
            except Exception as e:
                app.logger.info(f"[livemap] miss {url}: {e}")
    return {}, 0

# ï¼ˆå¯é¸ï¼‰é€²ç¬¬äºŒæ­¥ç¥¨å€é è£œæŠ“æ•¸å­—
def fetch_area_left_from_utk0101(base_000_url: str, perf_id: str, product_id: str, area_id: str, sess: requests.Session) -> Optional[int]:
    try:
        url = "https://orders.ibon.com.tw/Application/UTK01/UTK0101_02.aspx"
        params = {
            "PERFORMANCE_ID": perf_id,
            "PERFORMANCE_PRICE_AREA_ID": area_id,
            "PRODUCT_ID": product_id,
            "strItem": "WEBç¶²ç«™å…¥æ‰‹A1",
        }
        headers = {"Referer": base_000_url, "User-Agent": UA, "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.6"}
        r = sess.get(url, params=params, headers=headers, timeout=12)
        if r.status_code != 200:
            return None
        html = r.text
        m = re.search(r'(?:å‰©é¤˜|å°šé¤˜|å¯è³¼è²·|å¯å”®)[^\d]{0,6}(\d{1,3})', html)
        if not m:
            m = re.search(r'(\d{1,3})\s*å¼µ', html)
        if m:
            return int(m.group(1))

        soup = soup_parse(html)
        qty = None
        for inp in soup.select('input[type="number"],input[name*="QTY" i],select[name*="QTY" i]'):
            for attr in ("max", "data-max", "data-left", "data-remain"):
                v = inp.get(attr)
                if v and str(v).isdigit():
                    qty = max(qty or 0, int(v))
        return qty
    except Exception as e:
        app.logger.info(f"[area-left] fail {area_id}: {e}")
        return None

# --------- ä¸»è¦è§£æå™¨ ---------
def parse_UTK0201_000(url: str, sess: requests.Session) -> dict:
    out = {"ok": False, "sig": "NA", "url": url, "image": LOGO}
    r = sess.get(url, timeout=15)
    if r.status_code != 200:
        out["msg"] = f"è®€å–å¤±æ•—ï¼ˆHTTP {r.status_code}ï¼‰"
        return out
    html = r.text
    soup = soup_parse(html)

    q = parse_qs(urlparse(url).query)
    perf_id = (q.get("PERFORMANCE_ID") or [None])[0]
    product_id = (q.get("PRODUCT_ID") or [None])[0]

    # åœ–ç‰‡
    poster_from_000, seatmap = pick_event_images_from_000(html, url)
    if seatmap: out["seatmap"] = seatmap

    # æ´»å‹•åŸºæœ¬è³‡è¨Š
    api_info: Dict[str, str] = {}
    try:
        api_info = fetch_game_info_from_api(perf_id, product_id, url, sess)
    except Exception as e:
        app.logger.info(f"[api] fail: {e}")

    html_title, html_place, html_dt = extract_title_place_from_html(html)

    html_details = find_details_url_candidates_from_html(html, url)
    details_url = (
        (html_details[0] if html_details else None)
        or api_info.get("details")
        or (PROMO_DETAILS_MAP.get(perf_id) if perf_id else None)
    )
    details_info: Dict[str, str] = {}
    if details_url:
        details_info = fetch_from_ticket_details(details_url, sess)

    chosen_img = (
        (PROMO_IMAGE_MAP.get(perf_id) if perf_id else None)
        or details_info.get("poster")
        or api_info.get("poster")
        or poster_from_000
        or LOGO
    )
    if not _url_ok(chosen_img):
        app.logger.info(f"[image] chosen invalid, fallback: {chosen_img}")
        chosen_img = seatmap if seatmap and _url_ok(seatmap) else LOGO
    out["image"] = chosen_img

    out["title"] = details_info.get("title") or api_info.get("title") or html_title or "ï¼ˆæœªå–åˆ°æ¨™é¡Œï¼‰"
    out["place"] = details_info.get("place") or api_info.get("place") or html_place or "ï¼ˆæœªå–åˆ°å ´åœ°ï¼‰"
    out["date"]  = details_info.get("dt")    or api_info.get("dt")    or html_dt    or "ï¼ˆæœªå–åˆ°æ—¥æœŸï¼‰"

    # ç¥¨å€ä¸­æ–‡å + ç‹€æ…‹ï¼ˆAMOUNTï¼‰+ é †åº
    area_name_map, area_status_map, area_qty_map, area_order_map = extract_area_meta_from_000(html)
    out["area_names"] = area_name_map

    # live.map æ•¸å­—ï¼ˆåƒ…å–å¯ä¿¡æ•¸å­—ï¼Œä¸”åŒå€å–æœ€å¤§å€¼ï¼‰
    sections_by_code, _ = try_fetch_livemap_by_perf(perf_id, sess, html=html)
    numeric_counts: Dict[str, int] = dict(sections_by_code)
    # è¡¨æ ¼ã€Œç©ºä½ã€æ•¸å­—è£œé€²ä¾†ï¼ˆlive.map æ²’æœ‰çš„æ‰è£œï¼‰
    for code, n in area_qty_map.items():
        if isinstance(n, int) and n > 0 and code not in numeric_counts:
            numeric_counts[code] = n

    # åªçŸ¥é“ã€Œç†±è³£ä¸­/å¯å”®ã€ä½†æ²’æ•¸å­—
    selling_unknown_codes: List[str] = []
    for code, status in area_status_map.items():
        if (status and ("ç†±è³£" in status or "å¯å”®" in status)) and not numeric_counts.get(code):
            selling_unknown_codes.append(code)

    # é‡å°ã€Œç†±è³£ä¸­ã€ä½†æ²’æœ‰æ•¸å­—çš„å€ï¼šæ˜¯å¦é€²ç¬¬äºŒæ­¥é è£œæŠ“ï¼Ÿ
    if FOLLOW_AREAS_PER_CHECK > 0 and perf_id and product_id and area_name_map:
        need_follow = [code for code, st in area_status_map.items()
                       if (st and "ç†±è³£" in st) and (code not in numeric_counts)]
        for code in need_follow[:FOLLOW_AREAS_PER_CHECK]:
            n = fetch_area_left_from_utk0101(url, perf_id, product_id, code, sess)
            if isinstance(n, int) and n > 0:
                numeric_counts[code] = n

    # å†æ¬¡èšé›† selling_unknownï¼ˆç¢ºä¿è¦†è“‹å¾Œä»ç‚ºæœªçŸ¥ï¼‰
    selling_unknown_codes = [
        code for code, amt in area_status_map.items()
        if (amt and ("ç†±è³£" in amt or "å¯å”®" in amt)) and not numeric_counts.get(code)
    ]

    # ==== èšåˆè¼¸å‡º ====
    # æŠŠä»£ç¢¼æ˜ æˆäººé¡åç¨±ï¼ˆåŒåå€å–æœ€å¤§å€¼ï¼›ä¸æŠŠã€Œç†±è³£ä¸­(æœªçŸ¥)ã€ç®—é€² totalï¼‰
    human_numeric: Dict[str, int] = {}
    for code, n in numeric_counts.items():
        name = area_name_map.get(code, code)
        v = int(n)
        human_numeric[name] = max(human_numeric.get(name, 0), v)

    # é †åºï¼šä¾ç¶²ç«™é †åºï¼ˆSORT æˆ–è¡¨æ ¼åˆ—åºï¼‰
    def order_key(name: str) -> tuple:
        codes = [c for c, nm in area_name_map.items() if nm == name]
        order_vals = [area_order_map.get(c, 99999) for c in codes] or [99999]
        return (min(order_vals), name)

    ordered_names = sorted(human_numeric.keys(), key=order_key)
    selling_names = sorted({area_name_map.get(code, code) for code in selling_unknown_codes}, key=order_key)

    total_num = sum(human_numeric.values())

    # ---- å”®å®Œåµæ¸¬ï¼šæ‰€æœ‰ç¥¨å€çš†ã€Œå·²å”®å®Œã€ï¼Œä¸”æ²’æœ‰æ•¸å­—èˆ‡ã€Œç†±è³£/å¯å”®ã€æ¨™è¨˜ ----
    sold_out = False
    if area_name_map:
        any_hot = any(("ç†±è³£" in s) or ("å¯å”®" in s) or ("å¯è³¼" in s) for s in area_status_map.values())
        any_num = any(v > 0 for v in numeric_counts.values())
        if not any_hot and not any_num and area_status_map:
            sold_out = all(("å·²å”®å®Œ" in area_status_map.get(code, "")) for code in area_name_map.keys())

    out["sections"] = human_numeric
    out["sections_order"] = ordered_names
    out["selling"] = selling_names
    out["total"] = total_num
    out["soldout"] = bool(sold_out)

    # ç°½ç« ï¼šæŠŠå”®å®Œç‹€æ…‹ä¹Ÿç´å…¥ï¼Œé¿å…ã€Œå”®å®Œ <-> æœ‰ç¥¨ã€æ™‚ä¸è§¸ç™¼é€šçŸ¥
    sig_base = hash_state(human_numeric, selling_names)
    out["sig"] = hashlib.md5((sig_base + ("|SO" if sold_out else "")).encode("utf-8")).hexdigest()

    out["ok"] = (total_num > 0) or bool(selling_names)

    if out["ok"]:
        lines = [f"ğŸ« {out['title']}",
                 f"åœ°é»ï¼š{out['place']}",
                 f"æ—¥æœŸï¼š{out['date']}",
                 ""]
        if total_num > 0:
            lines.append("âœ… ç›£çœ‹çµæœï¼šç›®å‰å¯å”®")
            for name in ordered_names:
                lines.append(f"{name}: {human_numeric[name]} å¼µ")
            lines.append(f"åˆè¨ˆï¼š{total_num} å¼µ")
        if selling_names:
            if total_num > 0:
                lines.append("")  # åˆ†æ®µ
            lines.append("ğŸŸ¢ ç›®å‰ç†±è³£ä¸­ï¼ˆæ•¸é‡æœªå…¬é–‹ï¼‰ï¼š")
            for n in selling_names:
                lines.append(f"ãƒ»{n}ï¼ˆç†±è³£ä¸­ï¼‰")
        lines.append(out["url"])
        out["msg"] = "\n".join(lines)
        return out

    # ---- ç„¡æ•¸å­—ã€ç„¡ç†±è³£ï¼šè‹¥åˆ¤å®šç‚ºå”®å®Œ â†’ å›è¦†å”®å®Œï¼›å¦å‰‡ä¿ç•™åŸæœ¬èªªæ˜ ----
    if sold_out:
        out["msg"] = (
            f"ğŸ« {out['title']}\n"
            f"ğŸ“åœ°é»ï¼š{out['place']}\n"
            f"ğŸ“…æ—¥æœŸï¼š{out['date']}\n\n"
            f"ğŸ”´ å…¨å€å·²å”®å®Œ\n"
            f"{url}"
        )
        return out

    out["msg"] = (
        f"ğŸ« {out['title']}\n"
        f"åœ°é»ï¼š{out['place']}\n"
        f"æ—¥æœŸï¼š{out['date']}\n\n"
        "æš«æ™‚è®€ä¸åˆ°å‰©é¤˜æ•¸ï¼ˆå¯èƒ½ç‚ºå‹•æ…‹è¼‰å…¥ï¼‰ã€‚\n"
        f"{url}"
    )
    return out

def probe(url: str) -> dict:
    s = sess_default()
    p = urlparse(url)
    if "orders.ibon.com.tw" in p.netloc and p.path.upper().endswith("/UTK0201_000.ASPX"):
        return parse_UTK0201_000(url, s)
    r = s.get(url, timeout=12)
    title = ""
    try:
        soup = soup_parse(r.text)
        if soup.title and soup.title.text:
            title = soup.title.text.strip()
    except Exception:
        pass
    return {
        "ok": False, "sig": "NA", "url": url, "image": LOGO,
        "title": title or "ï¼ˆæœªå–åˆ°æ¨™é¡Œï¼‰", "place": "", "date": "", "msg": url,
    }

# ============= LINE æŒ‡ä»¤ =============
HELP = (
    "è¦ªæ„›çš„ç”¨æˆ¶æ‚¨å¥½ ğŸ–ï¸\n"
    "æ­¡è¿ä¾†åˆ°å·´æ‹‰åœ­ã®å°ˆå±¬æ¶ç¥¨åŠ©æ‰‹ ğŸ¤—\n"
    "æˆ‘æ˜¯æ‚¨çš„ç¥¨åˆ¸ç›£çœ‹æ©Ÿå™¨äºº ğŸ¤–\n\n"
    "æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤ä¾†é€²è¡Œæ“ä½œï¼š ğŸ‘‡\n\n"
    "âŠ/start æˆ– /help ï¼ é¡¯ç¤ºæ“ä½œèªªæ˜\n"
    "â‹/watch <URL> [ç§’] ï¼ é–‹å§‹ç›£çœ‹ï¼ˆæœ€å° 15 ç§’ï¼‰\n"
    "âŒ/unwatch <ä»»å‹™ID> ï¼ åœç”¨ä»»å‹™\n"
    "â/list ï¼ é¡¯ç¤ºå•Ÿç”¨ä¸­ä»»å‹™ï¼ˆ/list all çœ‹å…¨éƒ¨ã€/list off çœ‹åœç”¨ï¼‰\n"
    "â/check <URL|ä»»å‹™ID> ï¼ ç«‹åˆ»æ‰‹å‹•æŸ¥è©¢è©²é å‰©é¤˜æ•¸\n"
    "â/probe <URL> ï¼ å›å‚³è¨ºæ–· JSONï¼ˆé™¤éŒ¯ç”¨ï¼‰\n\n"

    "âibonå”®ç¥¨ç¶²ç«™é¦–é é€£çµ:https://ticket.ibon.com.tw/Index/entertainment \n"
    "å°‡ç”¨æˆ¶æƒ³è¿½è¹¤çš„ibonå”®ç¥¨ç¶²ç«™é€£çµè²¼å…¥<URL>æ¬„ä½å³å¯ \n"
    "ğŸ¤–ä»»å‹™IDæœƒåœ¨ç”¨æˆ¶è¼¸å…¥/watché–‹å§‹ç›£çœ‹å¾Œç”Ÿæˆä¸€å€‹å…­ä½æ•¸çš„ä»£ç¢¼ ğŸ¤–\n"
)
WELCOME_TEXT = HELP

# === å…¨åŸŸåªå›è¦†æŒ‡ä»¤ï¼šæ–°å¢åˆ¤æ–·ï¼ˆåŠå½¢ / å…¨å½¢æ–œç·šï¼‰ ===
CMD_PREFIX = ("/", "ï¼")
def is_command(text: Optional[str]) -> bool:
    if not text:
        return False
    return text.strip().startswith(CMD_PREFIX)

def source_id(ev):
    src = ev.source
    return getattr(src, "user_id", None) or getattr(src, "group_id", None) or getattr(src, "room_id", None) or ""

def make_task_id() -> str:
    return uuid.uuid4().hex[:6]

def fs_get_task_by_canon(chat_id: str, url_canon: str):
    if not FS_OK: return None
    q = (fs_client.collection(COL)
         .where("chat_id", "==", chat_id)
         .where("url_canon", "==", url_canon)
         .limit(1).stream())
    for d in q: return d
    return None

def fs_get_task_by_id(chat_id: str, tid: str):
    if not FS_OK: return None
    q = (fs_client.collection(COL)
         .where("chat_id", "==", chat_id)
         .where("id", "==", tid)
         .limit(1).stream())
    for d in q: return d
    return None

def fs_upsert_watch(chat_id: str, url: str, sec: int):
    if not FS_OK:
        raise RuntimeError("Firestore not available")
    url_c = canonicalize_url(url)
    sec = max(15, int(sec))
    now = datetime.now(timezone.utc)
    doc = fs_get_task_by_canon(chat_id, url_c)
    if doc:
        fs_client.collection(COL).document(doc.id).update({
            "period": sec, "enabled": True, "updated_at": now,
        })
        return doc.to_dict()["id"], False
    tid = make_task_id()
    fs_client.collection(COL).add({
        "id": tid, "chat_id": chat_id, "url": url, "url_canon": url_c,
        "period": sec, "enabled": True, "created_at": now, "updated_at": now,
        "last_sig": "", "last_total": 0, "last_ok": False, "next_run_at": now,
    })
    return tid, True

def fs_list(chat_id: str, show: str = "on"):
    if not FS_OK:
        return []

    base = fs_client.collection(COL).where("chat_id", "==", chat_id)
    if show == "on":
        base = base.where("enabled", "==", True)
    elif show == "off":
        base = base.where("enabled", "==", False)

    try:
        cur = base.order_by("updated_at", direction=firestore.Query.DESCENDING).stream()
        rows = []
        for d in cur:
            rows.append(d.to_dict())
        return rows
    except Exception as e:
        app.logger.info(f"[fs_list] order_by stream failed, fallback to unsorted: {e}")

    try:
        rows = [d.to_dict() for d in base.stream()]
        def _k(x):
            v = x.get("updated_at")
            return v if v is not None else datetime.fromtimestamp(0, timezone.utc)
        rows.sort(key=_k, reverse=True)
        return rows
    except Exception as e2:
        app.logger.error(f"[fs_list] fallback stream failed: {e2}")
        return []

def fs_disable(chat_id: str, tid: str) -> bool:
    doc = fs_get_task_by_id(chat_id, tid)
    if not doc: return False
    fs_client.collection(COL).document(doc.id).update({
        "enabled": False, "updated_at": datetime.now(timezone.utc),
    })
    return True

def fmt_result_text(res: dict) -> str:
    lines = []
    if res.get("task_id"):
        lines.append(f"ä»»å‹™ä»£ç¢¼ï¼š{res['task_id']}")
    lines += [
        f"ğŸ« {res.get('title','')}".strip(),
        f"ğŸ“åœ°é»ï¼š{res.get('place','')}",
        f"ğŸ“…æ—¥æœŸï¼š{res.get('date','')}",
    ]
    # è‹¥ç‚ºå”®å®Œç‹€æ…‹ï¼Œå„ªå…ˆè¼¸å‡ºå”®å®Œè¨Šæ¯
    if res.get("soldout"):
        lines.append("\nğŸ”´ å…¨å€å·²å”®å®Œ")
        lines.append(res.get("url", ""))
        return "\n".join(lines)

    if res.get("ok"):
        secs = res.get("sections", {})
        order = res.get("sections_order") or []
        selling = res.get("selling", [])
        if secs:
            lines.append("\nâœ… ç›£çœ‹çµæœï¼šç›®å‰å¯å”®")
            if order:
                for name in order:
                    if name in secs:
                        lines.append(f"{name}: {secs[name]} å¼µ")
            else:
                for k, v in sorted(secs.items(), key=lambda x: (-x[1], x[0])):
                    lines.append(f"{k}: {v} å¼µ")
            lines.append(f"åˆè¨ˆï¼š{res.get('total',0)} å¼µ")
        if selling:
            lines.append("\nğŸŸ¢ ç›®å‰ç†±è³£ä¸­ï¼ˆæ•¸é‡æœªå…¬é–‹ï¼‰ï¼š")
            for n in selling:
                lines.append(f"ãƒ»{n}ï¼ˆç†±è³£ä¸­ï¼‰")
    else:
        lines.append("\næš«æ™‚è®€ä¸åˆ°å‰©é¤˜æ•¸ï¼ˆå¯èƒ½ç‚ºå‹•æ…‹è¼‰å…¥ï¼‰ã€‚")
    lines.append(res.get("url", ""))
    return "\n".join(lines)

def handle_command(text: str, chat_id: str):
    try:
        parts = text.strip().split()
        cmd = parts[0].lower()
        if cmd in ("/start", "/help"):
            return [TextSendMessage(text=HELP)] if HAS_LINE else [HELP]

        if cmd == "/watch" and len(parts) >= 2:
            url = parts[1].strip()
            sec = int(parts[2]) if len(parts) >= 3 and parts[2].isdigit() else DEFAULT_PERIOD_SEC
            tid, created = fs_upsert_watch(chat_id, url, sec)
            status = "å•Ÿç”¨" if created else "æ›´æ–°"
            msg = f"ä½ çš„ä»»å‹™ï¼š\n{tid}ï½œ{status}ï½œ{sec}s\n{canonicalize_url(url)}"
            return [TextSendMessage(text=msg)] if HAS_LINE else [msg]

        if cmd == "/unwatch" and len(parts) >= 2:
            ok = fs_disable(chat_id, parts[1].strip())
            msg = "å·²åœç”¨" if ok else "æ‰¾ä¸åˆ°è©²ä»»å‹™"
            return [TextSendMessage(text=msg)] if HAS_LINE else [msg]

        if cmd == "/list":
            try:
                mode = "on"
                if len(parts) >= 2 and parts[1].lower() in ("all", "off"):
                    mode = parts[1].lower()

                rows = fs_list(chat_id, show=mode)
                if not rows:
                    out = "ï¼ˆæ²’æœ‰ä»»å‹™ï¼‰"
                    return [TextSendMessage(text=out)] if HAS_LINE else [out]

                chunks = []
                buf = "ä½ çš„ä»»å‹™ï¼š\n"
                for r in rows:
                    try:
                        rid    = str(r.get("id", "?"))
                        state  = "å•Ÿç”¨" if r.get("enabled") else "åœç”¨"
                        period = str(r.get("period", "?"))
                        u      = str(r.get("url", ""))
                        line = f"{rid}ï½œ{state}ï½œ{period}s\n{u}\n\n"
                    except Exception as e:
                        app.logger.info(f"[list] format row fail: {e}; row={r}")
                        line = f"{r}\n\n"

                    if len(buf) + len(line) > 4800:
                        chunks.append(buf.rstrip())
                        buf = ""
                    buf += line
                if buf:
                    chunks.append(buf.rstrip())

                if HAS_LINE:
                    to_reply = chunks[:5]
                    to_push  = chunks[5:]
                    msgs = [TextSendMessage(text=c) for c in to_reply]
                    for c in to_push:
                        try:
                            send_text(chat_id, c)
                        except Exception as e:
                            app.logger.error(f"[list] push remainder failed: {e}")
                    return msgs
                else:
                    return chunks

            except Exception as e:
                app.logger.error(f"/list failed: {e}\n{traceback.format_exc()}")
                out = "ï¼ˆè®€å–ä»»å‹™æ¸…å–®æ™‚ç™¼ç”Ÿä¾‹å¤–ï¼‰"
                return [TextSendMessage(text=out)] if HAS_LINE else [out]

        if cmd == "/check" and len(parts) >= 2:
            target = parts[1].strip()
            tid_for_msg = None
            if target.lower().startswith("http"):
                url = target
            else:
                doc = fs_get_task_by_id(chat_id, target)
                if not doc:
                    msg = "æ‰¾ä¸åˆ°è©²ä»»å‹™ ID"
                    return [TextSendMessage(text=msg)] if HAS_LINE else [msg]
                url = doc.to_dict().get("url")
                tid_for_msg = target

            res = probe(url)
            if tid_for_msg:
                res["task_id"] = tid_for_msg

            if HAS_LINE:
                msgs = []
                sent = set()
                sm  = res.get("seatmap")
                img = res.get("image")
                # å…ˆåº§ä½åœ–ã€å¾Œå®£å‚³åœ–
                if sm and _url_ok(sm):
                    msgs.append(ImageSendMessage(original_content_url=sm, preview_image_url=sm))
                    sent.add(sm)
                if img and _url_ok(img) and img not in sent:
                    msgs.append(ImageSendMessage(original_content_url=img, preview_image_url=img))
                msgs.append(TextSendMessage(text=fmt_result_text(res)))
                return msgs
            else:
                return [fmt_result_text(res)]

        if cmd == "/probe" and len(parts) >= 2:
            url = parts[1].strip()
            res = probe(url)
            out = json.dumps(res, ensure_ascii=False)
            return [TextSendMessage(text=out)] if HAS_LINE else [out]

        return [TextSendMessage(text=HELP)] if HAS_LINE else [HELP]
    except Exception as e:
        app.logger.error(f"handle_command error: {e}\n{traceback.format_exc()}")
        msg = "æŒ‡ä»¤è™•ç†ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
        return [TextSendMessage(text=msg)] if HAS_LINE else [msg]

# ============= Webhook / Scheduler / Diag =============
@app.route("/webhook", methods=["POST"])
def webhook():
    if not (HAS_LINE and handler):
        app.logger.warning("Webhook invoked but handler not ready")
        return "OK", 200
    signature = request.headers.get("X-Line-Signature", "")
    body = request.get_data(as_text=True)
    try:
        handler.handle(body, signature)
    except InvalidSignatureError:
        app.logger.warning("InvalidSignature on /webhook")
        abort(400)
    return "OK"

if HAS_LINE and handler:

    @handler.add(FollowEvent)
    def on_follow(ev):
        # è¢«åŠ å…¥å¥½å‹æ™‚ï¼šå›è¦†ä¸€æ¬¡ï¼ˆç­‰åŒ /start å…§å®¹ï¼‰
        try:
            line_bot_api.reply_message(ev.reply_token, [TextSendMessage(text=WELCOME_TEXT)])
        except Exception as e:
            app.logger.error(f"[follow] reply failed: {e}")

    @handler.add(JoinEvent)
    def on_join(ev):
        # è¢«é‚€å…¥ç¾¤/èŠå¤©å®¤æ™‚ï¼šå›è¦†ä¸€æ¬¡ï¼ˆç­‰åŒ /start å…§å®¹ï¼‰
        try:
            line_bot_api.reply_message(ev.reply_token, [TextSendMessage(text=WELCOME_TEXT)])
        except Exception as e:
            app.logger.error(f"[join] reply failed: {e}")

    @handler.add(MessageEvent, message=TextMessage)
    def on_message(ev):
        # å…¨åŸŸè¦å‰‡ï¼šåªæœ‰ã€ŒæŒ‡ä»¤ã€ï¼ˆä»¥ / æˆ– ï¼ é–‹é ­ï¼‰æ‰å›è¦†ï¼Œå…¶é¤˜å¿½ç•¥
        raw = getattr(ev.message, "text", "") or ""
        text = raw.strip()
        if not is_command(text):
            app.logger.info(f"[IGNORED NON-COMMAND] chat={source_id(ev)} text={text!r}")
            return

        chat = source_id(ev)
        msgs = handle_command(text, chat)
        if isinstance(msgs, list) and msgs and not isinstance(msgs[0], str):
            line_bot_api.reply_message(ev.reply_token, msgs)
        else:
            line_bot_api.reply_message(ev.reply_token, [TextSendMessage(text=str(msgs))])

@app.route("/cron/tick", methods=["GET"])
def cron_tick():
    start = time.time()
    resp = {"ok": True, "processed": 0, "skipped": 0, "errors": []}
    try:
        if not FS_OK:
            resp["ok"] = False
            resp["errors"].append("No Firestore client")
            return jsonify(resp), 200

        now = datetime.now(timezone.utc)

        try:
            docs = list(fs_client.collection(COL).where("enabled", "==", True).stream())
        except Exception as e:
            app.logger.error(f"[tick] list watchers failed: {e}")
            resp["ok"] = False
            resp["errors"].append(f"list failed: {e}")
            return jsonify(resp), 200

        handled = 0
        for d in docs:
            if (time.time() - start) > TICK_SOFT_DEADLINE_SEC:
                resp["errors"].append("soft-deadline reached; remaining will run next tick")
                break
            if handled >= MAX_PER_TICK:
                resp["errors"].append("max-per-tick reached; remaining will run next tick")
                break

            r = d.to_dict()
            period = int(r.get("period", DEFAULT_PERIOD_SEC))
            next_run_at = r.get("next_run_at") or (now - timedelta(seconds=1))
            if now < next_run_at:
                resp["skipped"] += 1
                continue

            url = r.get("url")
            try:
                res = probe(url)
            except Exception as e:
                app.logger.error(f"[tick] probe error for {url}: {e}")
                res = {"ok": False, "msg": f"probe error: {e}", "sig": "NA", "url": url}

            try:
                fs_client.collection(COL).document(d.id).update({
                    "last_sig": res.get("sig", "NA"),
                    "last_total": res.get("total", 0),
                    "last_ok": bool(res.get("ok", False)),
                    "updated_at": now,
                    "next_run_at": now + timedelta(seconds=period),
                })
            except Exception as e:
                app.logger.error(f"[tick] update doc error: {e}")
                resp["errors"].append(f"update error: {e}")

            changed = (res.get("sig", "NA") != r.get("last_sig", ""))
            if ALWAYS_NOTIFY or changed:
                try:
                    res["task_id"] = r.get("id")
                    chat_id = r.get("chat_id")
                    sent = set()
                    sm = res.get("seatmap")
                    img = res.get("image")
                    if sm and _url_ok(sm):
                        send_image(chat_id, sm); sent.add(sm)
                    if img and _url_ok(img) and img not in sent:
                        send_image(chat_id, img)
                    send_text(chat_id, fmt_result_text(res))
                except Exception as e:
                    app.logger.error(f"[tick] notify error: {e}")
                    resp["errors"].append(f"notify error: {e}")

            handled += 1
            resp["processed"] += 1

        app.logger.info(f"[tick] processed={resp['processed']} skipped={resp['skipped']} "
                        f"errors={len(resp['errors'])} duration={time.time()-start:.1f}s")
        return jsonify(resp), 200

    except Exception as e:
        app.logger.error(f"[tick] fatal: {e}\n{traceback.format_exc()}")
        resp["ok"] = False
        resp["errors"].append(str(e))
        return jsonify(resp), 200

@app.route("/diag", methods=["GET"])
def diag():
    url = request.args.get("url", "").strip()
    if not url:
        return jsonify({"ok": False, "msg": "missing url"}), 400
    try:
        res = probe(url)
        return jsonify(res), 200
    except Exception as e:
        return jsonify({"ok": False, "msg": str(e)}), 500

@app.route("/healthz", methods=["GET"])
def healthz():
    return "ok", 200

@app.route("/check", methods=["GET"])
def http_check_once():
    url = request.args.get("url", "").strip()
    if not url:
        return jsonify({"ok": False, "msg": "provide ?url=<UTK0201_000 url>"}), 400
    res = probe(url)
    return jsonify(res), 200

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", "8080")))

def fetch_ibon_entertainments(limit=10, keyword=None):
    url = "https://ticket.ibon.com.tw/Index/entertainment"
    try:
        r = requests.get(url, timeout=12, headers={
            "User-Agent": "Mozilla/5.0",
            "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.6",
        })
        r.raise_for_status()
        try:
            soup = BeautifulSoup(r.text, "lxml")
        except Exception:
            soup = BeautifulSoup(r.text, "html.parser")

        items, seen = [], set()
        for a in soup.select('a[href*="/ActivityInfo/Details/"]'):
            href = urljoin(url, (a.get("href") or "").strip())
            if href in seen:
                continue
            seen.add(href)
            title = (a.get("title") or a.get_text(" ", strip=True) or "æ´»å‹•").strip()
            if keyword and keyword not in title:
                continue
            img = None
            img_tag = a.select_one("img")
            if img_tag and img_tag.get("src"):
                img = urljoin(url, img_tag["src"])
            items.append({"title": title, "url": href, "image": img})
            if len(items) >= max(1, int(limit)):
                break
        return items
    except Exception as e:
        app.logger.error(f"[ent] fetch failed: {e}")
        return []

@app.route("/liff/activities", methods=["GET"])
def liff_activities():
    try:
        limit = int(request.args.get("limit", "10"))
    except Exception:
        limit = 10
    kw = request.args.get("q") or None
    try:
        acts = fetch_ibon_entertainments(limit=limit, keyword=kw)
        return jsonify(acts), 200
    except Exception as e:
        app.logger.error(f"/liff/activities error: {e}")
        return jsonify({"ok": False, "error": str(e)}), 500

@app.route("/liff/", methods=["GET"])
def liff_index():
    return send_from_directory("liff", "index.html")