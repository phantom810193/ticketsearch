# app.py
import os
import re
import json
import time
import uuid
import hashlib
import logging
import traceback
from datetime import datetime, timezone, timedelta
from urllib.parse import urlparse, urlunparse, parse_qs, urlencode, urljoin

import requests
from flask import Flask, request, abort, jsonify

# --------- LINE SDKï¼ˆå¯é¸ï¼‰---------
HAS_LINE = True
try:
    from linebot import LineBotApi, WebhookHandler
    from linebot.exceptions import InvalidSignatureError
    from linebot.models import MessageEvent, TextMessage, TextSendMessage, ImageSendMessage
except Exception as e:
    HAS_LINE = False
    LineBotApi = WebhookHandler = InvalidSignatureError = None
    MessageEvent = TextMessage = TextSendMessage = ImageSendMessage = None
    logging.warning(f"[init] line-bot-sdk not available: {e}")

# --------- Firestoreï¼ˆå¯å¤±æ•—ä¸è‡´å‘½ï¼‰---------
from google.cloud import firestore

# HTML è§£æ
from bs4 import BeautifulSoup

app = Flask(__name__)
logging.basicConfig(level=os.getenv("LOG_LEVEL", "INFO"))
app.logger.setLevel(logging.INFO)

# ======== ç’°å¢ƒè®Šæ•¸ ========
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN", "")
LINE_CHANNEL_SECRET = os.getenv("LINE_CHANNEL_SECRET", "")
DEFAULT_PERIOD_SEC = int(os.getenv("DEFAULT_PERIOD_SEC", "60"))
ALWAYS_NOTIFY = os.getenv("ALWAYS_NOTIFY", "0") == "1"

if not LINE_CHANNEL_ACCESS_TOKEN or not LINE_CHANNEL_SECRET:
    app.logger.warning("LINE env not set: LINE_CHANNEL_ACCESS_TOKEN / LINE_CHANNEL_SECRET")

line_bot_api = LineBotApi(LINE_CHANNEL_ACCESS_TOKEN) if (HAS_LINE and LINE_CHANNEL_ACCESS_TOKEN) else None
handler = WebhookHandler(LINE_CHANNEL_SECRET) if (HAS_LINE and LINE_CHANNEL_SECRET) else None

MAX_PER_TICK = int(os.getenv("MAX_PER_TICK", "6"))                     # æ¯æ¬¡æœ€å¤šè™•ç†å¹¾å€‹ä»»å‹™
TICK_SOFT_DEADLINE_SEC = int(os.getenv("TICK_SOFT_DEADLINE_SEC", "50"))  # è»Ÿæ€§æˆªæ­¢(ç§’)

# Firestore client
try:
    fs_client = firestore.Client()
    FS_OK = True
except Exception as e:
    app.logger.warning(f"Firestore init failed: {e}")
    fs_client = None
    FS_OK = False

COL = "watchers"

UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/122.0 Safari/537.36"
)

_RE_DATE = re.compile(r"(\d{4}/\d{2}/\d{2}\s+\d{2}:\d{2})")
_RE_AREA_TAG = re.compile(r"<area\b[^>]*>", re.I)

# === æ–°å¢ï¼šæŠ“ä»£ç¢¼/æ•¸é‡ç”¨ ===
COUNT_CODE_RE = re.compile(r"\b(B0[0-9A-Z]{6,10})\b")
COUNT_PAIR_RE = re.compile(r"\b(B0[0-9A-Z]{6,10})\D{0,12}(\d{1,5})\s*å¼µ")
IBON_HOST_RE = re.compile(r"(^|\.)orders\.ibon\.com\.tw$", re.I)

LOGO = "https://ticketimg2.azureedge.net/logo.png"

# ================= å°å·¥å…· =================
def soup_parse(html: str) -> BeautifulSoup:
    """å„ªå…ˆç”¨ lxmlï¼Œæ²’æœ‰å†é€€å› html.parserã€‚"""
    try:
        return BeautifulSoup(html, "lxml")
    except Exception:
        return BeautifulSoup(html, "html.parser")

def now_ts() -> float:
    return time.time()

def hash_sections(d: dict) -> str:
    items = sorted((k, int(v)) for k, v in d.items())
    raw = json.dumps(items, ensure_ascii=False, separators=(",", ":"))
    return hashlib.md5(raw.encode("utf-8")).hexdigest()

def canonicalize_url(u: str) -> str:
    p = urlparse(u.strip())
    q = parse_qs(p.query, keep_blank_values=True)
    q_sorted = []
    for k in sorted(q.keys()):
        for v in q[k]:
            q_sorted.append((k, v))
    new_q = urlencode(q_sorted, doseq=True)
    return urlunparse((p.scheme, p.netloc, p.path, "", new_q, ""))

def send_text(to_id: str, text: str):
    if not line_bot_api:
        app.logger.info(f"[dry-run] send_text to {to_id}: {text}")
        return
    try:
        line_bot_api.push_message(to_id, TextSendMessage(text=text))
    except Exception as e:
        app.logger.error(f"[LINE] push text failed: {e}")

def send_image(to_id: str, img_url: str):
    if not line_bot_api:
        app.logger.info(f"[dry-run] send_image to {to_id}: {img_url}")
        return
    try:
        line_bot_api.push_message(
            to_id,
            ImageSendMessage(original_content_url=img_url, preview_image_url=img_url)
        )
    except Exception as e:
        app.logger.error(f"[LINE] push image failed: {e}")

def sess_default() -> requests.Session:
    s = requests.Session()
    s.headers.update({
        "User-Agent": UA,
        "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.6",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Connection": "close",
    })
    return s

# ============= å…±åŒè§£æï¼šæ¨™é¡Œ/åœ°é»/æ—¥æœŸ/åœ–ç‰‡ =============
def pick_event_image_from_000(html: str, base_url: str) -> str:
    """å¾ 000 é é¢æŒ‘ä¸€å¼µæ´»å‹•åœ–ï¼šog:image / twitter:image / å…§åµŒå« azureedge | ActivityImage | static_bigmap"""
    try:
        soup = soup_parse(html)
        for sel in ['meta[property="og:image"]', 'meta[name="twitter:image"]']:
            m = soup.select_one(sel)
            if m and m.get("content"):
                return urljoin(base_url, m["content"])

        urls = []
        for img in soup.find_all("img"):
            if img.get("src"):
                urls.append(img["src"])
            if img.get("srcset"):
                urls.extend([p.split()[0] for p in img["srcset"].split(",") if p.strip()])

        urls += re.findall(r'https?://[^\s"\'<>]+\.(?:jpg|jpeg|png)', html, flags=re.I)

        for u in urls:
            lu = u.lower()
            if any(key in lu for key in ["azureedge", "activityimage", "static_bigmap", "bigmap", "image"]):
                return urljoin(base_url, u)
    except Exception as e:
        app.logger.warning(f"[image] pick failed: {e}")
    return LOGO

def pick_event_image_generic(html: str, base_url: str) -> str:
    """é€šç”¨é é¢çš„ä¸»åœ–ï¼šå…ˆ og:imageï¼Œå†æ‰¾æœ€å¤§å¼µåœ–ç‰‡"""
    try:
        soup = soup_parse(html)
        for sel in ['meta[property="og:image"]', 'meta[name="twitter:image"]']:
            m = soup.select_one(sel)
            if m and m.get("content"):
                return urljoin(base_url, m["content"])
        imgs = soup.find_all("img")
        best = None
        best_area = -1
        for img in imgs:
            src = img.get("src") or ""
            if not src:
                continue
            w = int(img.get("width") or 0) or 0
            h = int(img.get("height") or 0) or 0
            area = w * h
            if area > best_area:
                best = src; best_area = area
        if best:
            return urljoin(base_url, best)
    except Exception as e:
        app.logger.warning(f"[image] generic pick failed: {e}")
    return LOGO

def extract_area_name_map_from_000(html: str) -> dict:
    """å¾ UTK0201_000 è¡¨æ ¼æŠ½ {å€ä»£ç¢¼: ä¸­æ–‡åç¨±}ã€‚"""
    name_map = {}
    try:
        soup = soup_parse(html)
        for a in soup.select('a[href*="PERFORMANCE_PRICE_AREA_ID="]'):
            href = a.get("href", "")
            m = re.search(r'PERFORMANCE_PRICE_AREA_ID=([A-Za-z0-9]+)', href)
            if not m:
                continue
            code = m.group(1)
            tr = a.find_parent("tr")
            cand_text = ""
            if tr:
                tds = [td.get_text(" ", strip=True) for td in tr.find_all("td")]
                pick = None
                for t in tds:
                    if re.search(r"[A-Z0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+.*[å€åŒº]", t):
                        pick = t
                        break
                cand_text = pick or (tds[0] if tds else "")
            else:
                cand_text = a.get_text(strip=True) or a.get("title", "")
            cand_text = re.sub(r"\s+", "", cand_text)
            if cand_text:
                name_map[code] = cand_text
    except Exception as e:
        app.logger.warning(f"[area-map] extract failed: {e}")
    return name_map

def _parse_livemap_text(txt: str):
    """è§£æ azureedge live.mapã€‚"""
    sections = {}
    total = 0
    for tag in _RE_AREA_TAG.findall(txt):
        # å€ä»£ç¢¼/åç¨±
        name = "æœªå‘½åå€"
        m_href = re.search(
            r"javascript:Send\([^)]*'([A-Za-z0-9]+)'\s*,\s*'([A-Za-z0-9]+)'\s*,\s*'(\d+)'",
            tag, re.I)
        if m_href:
            name = m_href.group(2)

        # æ•¸é‡ï¼štitle å…§æœ€å¾Œä¸€å€‹ <1000 çš„æ•¸å­—
        qty = None
        m_title = re.search(r'title="([^"]*)"', tag, re.I)
        title_text = m_title.group(1) if m_title else ""
        nums = [int(n) for n in re.findall(r"(\d+)", title_text)]
        for n in reversed(nums):
            if n < 1000:
                qty = n
                break

        if qty is None:
            m = re.search(r'\bdata-(?:left|remain|qty|count)=["\']?(\d+)["\']?', tag, re.I)
            if m: qty = int(m.group(1))
        if qty is None:
            m = re.search(r'\b(?:alt|aria-label)=["\'][^"\']*?(\d+)[^"\']*["\']', tag, re.I)
            if m: qty = int(m.group(1))

        if not qty or qty <= 0:
            continue

        key = re.sub(r"\s+", "", name) or "æœªå‘½åå€"
        sections[key] = sections.get(key, 0) + qty
        total += qty
    return sections, total

def try_fetch_livemap_by_perf(perf_id: str, sess: requests.Session):
    """çŒœæ¸¬ live.map çš„ URLï¼Œå„ªå…ˆ 1_ å‰ç¶´ï¼›å‘½ä¸­å¾Œè§£æã€‚"""
    if not perf_id:
        return {}, 0
    pids = {perf_id, perf_id.upper(), perf_id.lower()}
    prefixes = ["1", "2", "3", "0", "4", "5", "01", "02", "03", ""]
    base = "https://qwareticket-asysimg.azureedge.net/QWARE_TICKET/images/Temp"
    for pid in pids:
        for pref in prefixes:
            prefix = f"{pref}_" if pref else ""
            url = f"{base}/{pid}/{prefix}{pid}_live.map"
            try:
                app.logger.info(f"[livemap] try {url}")
                r = sess.get(url, timeout=12)
                if r.status_code == 200 and "<area" in r.text:
                    app.logger.info(f"[livemap] guessed and hit: {url}")
                    return _parse_livemap_text(r.text)
            except Exception as e:
                app.logger.warning(f"[livemap] guess fail {url}: {e}")
    return {}, 0

# === æ–°å¢ï¼šé€šç”¨ counts è§£æï¼ˆæ–‡å­— & script JSONï¼‰ ===
def _parse_counts_from_text(text: str) -> dict:
    counts = {}
    # ç›´æ¥æŠ“ã€Œä»£ç¢¼ ... N å¼µã€
    for m in COUNT_PAIR_RE.finditer(text):
        code, n = m.group(1), int(m.group(2))
        counts[code] = max(n, counts.get(code, 0))
    # è‹¥æ²’æŠ“åˆ°ï¼Œå˜—è©¦é„°è¿‘è¡Œçš„ã€Œä»£ç¢¼ + N å¼µã€
    if not counts:
        lines = [x.strip() for x in text.splitlines() if x.strip()]
        for i, ln in enumerate(lines):
            m = COUNT_CODE_RE.search(ln)
            if not m:
                continue
            code = m.group(1)
            ctx = " ".join(lines[max(0, i-1): i+2])
            m2 = re.search(r"(\d{1,5})\s*å¼µ", ctx)
            if m2:
                counts[code] = max(int(m2.group(1)), counts.get(code, 0))
    return counts

def _parse_counts_from_scripts(soup: BeautifulSoup) -> dict:
    counts = {}
    for sc in soup.find_all("script"):
        sc_txt = (sc.string or sc.text or "").strip()
        if not sc_txt:
            continue
        # å…ˆç”¨æ­£å‰‡è£œæŠ“
        c2 = _parse_counts_from_text(sc_txt)
        for k, v in c2.items():
            counts[k] = max(v, counts.get(k, 0))
        # ç›¡é‡è§£æ JSON çµæ§‹
        if "{" in sc_txt and "}" in sc_txt and any(k in sc_txt.lower() for k in ("remain", "qty", "quantity", "left")):
            try:
                blob = sc_txt[sc_txt.find("{"): sc_txt.rfind("}") + 1]
                data = json.loads(blob)
            except Exception:
                data = None
            if isinstance(data, dict):
                stack = [data]
                while stack:
                    node = stack.pop()
                    if isinstance(node, dict):
                        code = None; qty = None
                        for k, v in node.items():
                            k_l = str(k).lower()
                            if isinstance(v, str) and COUNT_CODE_RE.fullmatch(v):
                                code = v
                            if isinstance(v, (int, str)) and k_l in ("remain", "remainqty", "qty", "quantity", "left", "remaincount"):
                                try: qty = int(v)
                                except: pass
                        if code and isinstance(qty, int):
                            counts[code] = max(qty, counts.get(code, 0))
                        stack.extend(node.values())
                    elif isinstance(node, list):
                        stack.extend(node)
    return counts

# === æ–°å¢ï¼šPlaywright å‹•æ…‹å‚™æ´ ===
def _try_dynamic_counts(event_url: str, timeout_sec: int = 20) -> dict:
    counts = {}
    try:
        from playwright.sync_api import sync_playwright
    except Exception as e:
        app.logger.info(f"[dyn] playwright not installed: {e}")
        return counts

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, args=["--no-sandbox", "--disable-setuid-sandbox", "--disable-dev-shm-usage"])
        ctx = browser.new_context(user_agent=UA, locale="zh-TW", java_script_enabled=True)
        page = ctx.new_page()

        def on_response(resp):
            try:
                ct = resp.headers.get("content-type", "")
                if "application/json" not in ct:
                    return
                if "ibon.com.tw" not in resp.url:
                    return
                data = resp.json()
            except Exception:
                return
            # æ·±åº¦æ‰¾ code+qty
            stack = [data]
            while stack:
                node = stack.pop()
                if isinstance(node, dict):
                    code = None; qty = None
                    for k, v in node.items():
                        k_l = str(k).lower()
                        if isinstance(v, str) and COUNT_CODE_RE.fullmatch(v):
                            code = v
                        if isinstance(v, (int, str)) and k_l in ("remain", "remainqty", "qty", "quantity", "left", "remaincount"):
                            try: qty = int(v)
                            except: pass
                    if code and isinstance(qty, int):
                        counts[code] = max(qty, counts.get(code, 0))
                    stack.extend(node.values())
                elif isinstance(node, list):
                    stack.extend(node)

        page.on("response", on_response)
        page.set_extra_http_headers({"User-Agent": UA, "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.6"})
        page.goto(event_url, wait_until="domcontentloaded")
        try:
            page.wait_for_load_state("networkidle", timeout=timeout_sec * 1000)
        except Exception:
            pass

        body_text = page.evaluate("document.body.innerText")
        cc = _parse_counts_from_text(body_text)
        for k, v in cc.items():
            counts[k] = max(v, counts.get(k, 0))

        html = page.content()
        soup = BeautifulSoup(html, "html.parser")
        cc2 = _parse_counts_from_text(soup.get_text("\n", strip=True))
        for k, v in cc2.items():
            counts[k] = max(v, counts.get(k, 0))

        ctx.close()
        browser.close()
    return counts

# === å€åŸŸåç¨±å°æ‡‰ï¼ˆæœ€é•·å‰ç¶´ï¼‰ ===
def map_counts_to_zones(counts: dict, area_map: dict) -> tuple[list[tuple[str, str, int]], list[tuple[str, int]]]:
    matched, unmatched = [], []
    for code, n in counts.items():
        name = area_map.get(code)
        if not name:
            best = None
            for k, v in area_map.items():
                if code.startswith(k) or k.startswith(code):
                    if best is None or len(k) > len(best[0]):
                        best = (k, v)
            if best:
                name = best[1]
        if name:
            matched.append((name, code, int(n)))
        else:
            unmatched.append((code, int(n)))
    matched.sort(key=lambda x: x[2], reverse=True)
    return matched, unmatched

# ============= ibon è§£æ =============
def parse_UTK0201_000(url: str, sess: requests.Session) -> dict:
    """è§£æ 000 é ï¼ŒæŠ“æ¨™é¡Œ/åœ°é»/æ—¥æœŸ/æ´»å‹•åœ– + live.map æˆ–é é¢æ–‡å­—ç¥¨æ•¸ã€‚"""
    out = {"ok": False, "sig": "NA", "url": url, "image": LOGO}
    r = sess.get(url, timeout=15)
    if r.status_code != 200:
        out["msg"] = f"è®€å–å¤±æ•—ï¼ˆHTTP {r.status_code}ï¼‰"
        return out
    html = r.text

    # æ¨™é¡Œ/åœ°é»/æ—¥æœŸ
    title = ""
    place = ""
    date_str = ""
    try:
        soup = soup_parse(html)
        # æ¨™é¡Œ
        m = soup.select_one("title")
        if m and m.text.strip():
            title = m.text.strip().replace("ibonå”®ç¥¨ç³»çµ±", "").strip()
        mt = soup.select_one('meta[property="og:title"]')
        if not title and mt and mt.get("content"):
            title = mt["content"].strip()
        # åœ°é»
        candidates = soup.find_all(string=re.compile(r"å ´åœ°|åœ°å€"))
        if candidates:
            for t in candidates:
                td = getattr(t, "parent", None)
                if not td:
                    continue
                tr = td.find_parent("tr")
                if tr:
                    tds = tr.find_all("td")
                    if len(tds) >= 3:
                        place = tds[2].get_text(" ", strip=True)
                        if place:
                            break
        # æ—¥æœŸ
        m = _RE_DATE.search(html)
        if m:
            date_str = m.group(1)
    except Exception as e:
        app.logger.warning(f"[parse000] meta fail: {e}")

    out["title"] = title or "ï¼ˆæœªå–åˆ°æ¨™é¡Œï¼‰"
    out["place"] = place or "ï¼ˆæœªå–åˆ°å ´åœ°ï¼‰"
    out["date"]  = date_str or "ï¼ˆæœªå–åˆ°æ—¥æœŸï¼‰"

    # ä¸»åœ–
    out["image"] = pick_event_image_from_000(html, url)

    # ç¥¨å€åç¨±æ˜ å°„
    area_name_map = extract_area_name_map_from_000(html)
    out["area_names"] = area_name_map

    # å…ˆè©¦ live.map
    q = parse_qs(urlparse(url).query)
    perf_id = (q.get("PERFORMANCE_ID") or [None])[0]
    sections_by_code, total = try_fetch_livemap_by_perf(perf_id, sess)

    # è‹¥ live.map æŠ“ä¸åˆ° â†’ ç›´æ¥å¾é é¢æ–‡å­—/è…³æœ¬è©¦æŠ“
    if total <= 0:
        soup = soup_parse(html)
        counts = _parse_counts_from_text(soup.get_text("\n", strip=True))
        if not counts:
            counts = _parse_counts_from_scripts(soup)
        if counts:
            # æŠŠä»£ç¢¼å°æ‡‰ä¸­æ–‡
            matched, unmatched = map_counts_to_zones(counts, area_name_map)
            human = {}
            for name, code, n in matched:
                human[name] = human.get(name, 0) + int(n)
            # æœªå°åˆ°çš„ä¿ç•™ä»£ç¢¼
            for code, n in unmatched:
                human[code] = human.get(code, 0) + int(n)
            total = sum(human.values())
            if total > 0:
                out["sections"] = human
                out["total"] = total
                out["ok"] = True
                out["sig"] = hash_sections(human)
                lines = [f"âœ… ç›£çœ‹çµæœï¼šç›®å‰å¯å”®"]
                for k, v in sorted(human.items(), key=lambda x: (-x[1], x[0])):
                    lines.append(f"{k}: {v} å¼µ")
                lines.append(f"åˆè¨ˆï¼š{total} å¼µ")
                out["msg"] = "\n".join(lines) + f"\n{url}"
                return out

    if total > 0:
        # live.map å‘½ä¸­ï¼šå€ä»£ç¢¼ â†’ ä¸­æ–‡åç¨±ï¼ˆè‹¥ç„¡å°æ‡‰å°±ç”¨åŸä»£ç¢¼ï¼‰
        human = {}
        for code, qty in sections_by_code.items():
            disp = area_name_map.get(code, code)
            human[disp] = human.get(disp, 0) + int(qty)
        out["sections"] = human
        out["total"] = total
        out["ok"] = True
        out["sig"] = hash_sections(human)
        lines = [f"âœ… ç›£çœ‹çµæœï¼šç›®å‰å¯å”®"]
        for k, v in sorted(human.items(), key=lambda x: (-x[1], x[0])):
            lines.append(f"{k}: {v} å¼µ")
        lines.append(f"åˆè¨ˆï¼š{total} å¼µ")
        out["msg"] = "\n".join(lines) + f"\n{url}"
    else:
        out["msg"] = (
            f"ğŸ« {out['title']}\n"
            f"åœ°é»ï¼š{out['place']}\n"
            f"æ—¥æœŸï¼š{out['date']}\n\n"
            "æš«æ™‚è®€ä¸åˆ°å‰©é¤˜æ•¸ï¼ˆå¯èƒ½ç‚ºå‹•æ…‹è¼‰å…¥ï¼‰ã€‚\n"
            f"{url}"
        )
    return out

def parse_ibon_generic(url: str, sess: requests.Session) -> dict:
    """
    é€šç”¨ ibon é è™•ç†ï¼š
    - è‹¥æ˜¯ 000 é  â†’ èµ° parse_UTK0201_000
    - å…¶å®ƒé ï¼šå…ˆéœæ…‹æŠ“ï¼ˆé é¢æ–‡å­— + scriptï¼‰ï¼ŒæŠ“ä¸åˆ°å†ç”¨ Playwright å‹•æ…‹å‚™æ´
    """
    p = urlparse(url)
    if p.path.upper().endswith("/UTK0201_000.ASPX"):
        return parse_UTK0201_000(url, sess)

    out = {"ok": False, "sig": "NA", "url": url, "image": LOGO}
    r = sess.get(url, timeout=15)
    if r.status_code != 200:
        out["msg"] = f"è®€å–å¤±æ•—ï¼ˆHTTP {r.status_code}ï¼‰"
        return out
    html = r.text
    soup = soup_parse(html)

    # æ¨™é¡Œ/åœ°é»/æ—¥æœŸ/åœ–ç‰‡ï¼ˆé€šç”¨ï¼‰
    title = (soup.title.text.strip() if soup.title and soup.title.text else "")
    if not title:
        mt = soup.select_one('meta[property="og:title"]')
        if mt and mt.get("content"): title = mt["content"].strip()
    out["title"] = title or "ï¼ˆæœªå–åˆ°æ¨™é¡Œï¼‰"

    place = ""
    for t in soup.find_all(string=re.compile(r"å ´åœ°|åœ°å€|åœ°é»")):
        td = getattr(t, "parent", None)
        if not td: continue
        tr = td.find_parent("tr")
        if tr:
            tds = tr.find_all("td")
            if len(tds) >= 2:
                place = tds[-1].get_text(" ", strip=True)
                if place: break
    out["place"] = place or "ï¼ˆæœªå–åˆ°å ´åœ°ï¼‰"

    m = _RE_DATE.search(html)
    out["date"] = m.group(1) if m else "ï¼ˆæœªå–åˆ°æ—¥æœŸï¼‰"

    out["image"] = pick_event_image_generic(html, url)

    # å…ˆéœæ…‹æŠ“ counts
    counts = _parse_counts_from_text(soup.get_text("\n", strip=True))
    if not counts:
        counts = _parse_counts_from_scripts(soup)

    # ç¥¨å€åç¨±å­—å…¸ï¼ˆèƒ½æŠ“åˆ°å°±å°æ‡‰ï¼‰
    area_name_map = extract_area_name_map_from_000(html)  # æœ‰äº›é ä¹Ÿæ²¿ç”¨ç›¸åŒæ¨£å¼
    out["area_names"] = area_name_map

    if not counts:
        # é€€åˆ° Playwright å‹•æ…‹
        counts = _try_dynamic_counts(url)

    if counts:
        matched, unmatched = map_counts_to_zones(counts, area_name_map)
        human = {}
        for name, code, n in matched:
            human[name] = human.get(name, 0) + int(n)
        for code, n in unmatched:
            human[code] = human.get(code, 0) + int(n)
        total = sum(human.values())
        out["sections"] = human
        out["total"] = total
        out["ok"] = total > 0
        if out["ok"]:
            out["sig"] = hash_sections(human)
            lines = [f"âœ… ç›£çœ‹çµæœï¼šç›®å‰å¯å”®"]
            for k, v in sorted(human.items(), key=lambda x: (-x[1], x[0])):
                lines.append(f"{k}: {v} å¼µ")
            lines.append(f"åˆè¨ˆï¼š{total} å¼µ")
            out["msg"] = "\n".join(lines) + f"\n{url}"
            return out

    # éƒ½æŠ“ä¸åˆ°
    out["msg"] = (
        f"ğŸ« {out['title']}\n"
        f"åœ°é»ï¼š{out['place']}\n"
        f"æ—¥æœŸï¼š{out['date']}\n\n"
        "æš«æ™‚è®€ä¸åˆ°å‰©é¤˜æ•¸ï¼ˆå¯èƒ½ç‚ºå‹•æ…‹è¼‰å…¥ï¼‰ã€‚\n"
        f"{url}"
    )
    return out

def probe(url: str) -> dict:
    """å…¥å£ï¼šæ”¹ç‚ºæ”¯æ´æ‰€æœ‰ ibon é ã€‚é ibon ä»å›åŸºæœ¬è¨Šæ¯ã€‚"""
    s = sess_default()
    p = urlparse(url)
    if IBON_HOST_RE.search(p.netloc):
        return parse_ibon_generic(url, s)

    # å…¶ä»–ç¶²å€ï¼šåƒ…å›åŸºæœ¬è¨Šæ¯
    r = s.get(url, timeout=12)
    title = ""
    try:
        soup = soup_parse(r.text)
        if soup.title and soup.title.text:
            title = soup.title.text.strip()
    except Exception:
        pass
    return {
        "ok": False,
        "sig": "NA",
        "url": url,
        "image": LOGO,
        "title": title or "ï¼ˆæœªå–åˆ°æ¨™é¡Œï¼‰",
        "place": "",
        "date": "",
        "msg": url,
    }

# ============= LINE æŒ‡ä»¤ =============
HELP = (
    "æˆ‘æ˜¯ç¥¨åˆ¸ç›£çœ‹æ©Ÿå™¨äºº ğŸ¤–\n"
    "æŒ‡ä»¤ï¼š\n"
    "/start æˆ– /help ï¼ é¡¯ç¤ºé€™å€‹èªªæ˜\n"
    "/watch <URL> [ç§’] ï¼ é–‹å§‹ç›£çœ‹ï¼ˆåŒç¶²å€ä¸é‡è¤‡ï¼›ç§’æ•¸å¯æ›´æ–°ï¼›æœ€å° 15 ç§’ï¼‰\n"
    "/unwatch <ä»»å‹™ID> ï¼ åœç”¨ä»»å‹™\n"
    "/list ï¼ é¡¯ç¤ºå•Ÿç”¨ä¸­ä»»å‹™ï¼ˆ/list all çœ‹å…¨éƒ¨ã€/list off çœ‹åœç”¨ï¼‰\n"
    "/check <URL|ä»»å‹™ID> ï¼ ç«‹åˆ»æ‰‹å‹•æŸ¥è©¢è©²é å‰©é¤˜æ•¸\n"
    "/probe <URL> ï¼ å›å‚³è¨ºæ–· JSONï¼ˆé™¤éŒ¯ç”¨ï¼‰\n"
)

def source_id(ev):
    src = ev.source
    return getattr(src, "user_id", None) or getattr(src, "group_id", None) or getattr(src, "room_id", None) or ""

def make_task_id() -> str:
    return uuid.uuid4().hex[:6]

def fs_get_task_by_canon(chat_id: str, url_canon: str):
    if not FS_OK: return None
    q = (fs_client.collection(COL)
         .where("chat_id", "==", chat_id)
         .where("url_canon", "==", url_canon)
         .limit(1).stream())
    for d in q:
        return d
    return None

def fs_get_task_by_id(chat_id: str, tid: str):
    if not FS_OK: return None
    q = (fs_client.collection(COL)
         .where("chat_id", "==", chat_id)
         .where("id", "==", tid)
         .limit(1).stream())
    for d in q:
        return d
    return None

def fs_upsert_watch(chat_id: str, url: str, sec: int):
    if not FS_OK:
        raise RuntimeError("Firestore not available")
    url_c = canonicalize_url(url)
    sec = max(15, int(sec))
    now = datetime.now(timezone.utc)
    doc = fs_get_task_by_canon(chat_id, url_c)
    if doc:
        fs_client.collection(COL).document(doc.id).update({
            "period": sec,
            "enabled": True,
            "updated_at": now,
        })
        return doc.to_dict()["id"], False
    tid = make_task_id()
    fs_client.collection(COL).add({
        "id": tid,
        "chat_id": chat_id,
        "url": url,
        "url_canon": url_c,
        "period": sec,
        "enabled": True,
        "created_at": now,
        "updated_at": now,
        "last_sig": "",
        "last_total": 0,
        "last_ok": False,
        "next_run_at": now,
    })
    return tid, True

def fs_list(chat_id: str, show: str = "on"):
    if not FS_OK: return []
    q = fs_client.collection(COL).where("chat_id", "==", chat_id)
    if show == "on":
        q = q.where("enabled", "==", True)
    elif show == "off":
        q = q.where("enabled", "==", False)
    return [d.to_dict() for d in q.order_by("updated_at", direction=firestore.Query.DESCENDING).stream()]

def fs_disable(chat_id: str, tid: str) -> bool:
    doc = fs_get_task_by_id(chat_id, tid)
    if not doc: return False
    fs_client.collection(COL).document(doc.id).update({
        "enabled": False,
        "updated_at": datetime.now(timezone.utc),
    })
    return True

def fmt_result_text(res: dict) -> str:
    lines = [f"ğŸ« {res.get('title','')}".strip(),
             f"åœ°é»ï¼š{res.get('place','')}",
             f"æ—¥æœŸï¼š{res.get('date','')}"]
    if res.get("ok"):
        lines.append("\nâœ… ç›£çœ‹çµæœï¼šç›®å‰å¯å”®")
        secs = res.get("sections", {})
        for k, v in sorted(secs.items(), key=lambda x: (-x[1], x[0])):
            lines.append(f"{k}: {v} å¼µ")
        lines.append(f"åˆè¨ˆï¼š{res.get('total',0)} å¼µ")
    else:
        lines.append("\næš«æ™‚è®€ä¸åˆ°å‰©é¤˜æ•¸ï¼ˆå¯èƒ½ç‚ºå‹•æ…‹è¼‰å…¥ï¼‰ã€‚")
    lines.append(res.get("url", ""))
    return "\n".join(lines)

def handle_command(text: str, chat_id: str):
    try:
        parts = text.strip().split()
        cmd = parts[0].lower()
        if cmd in ("/start", "/help"):
            return [TextSendMessage(text=HELP)] if HAS_LINE else [fmt_result_text({"msg": HELP})]

        if cmd == "/watch" and len(parts) >= 2:
            url = parts[1].strip()
            sec = int(parts[2]) if len(parts) >= 3 and parts[2].isdigit() else DEFAULT_PERIOD_SEC
            tid, created = fs_upsert_watch(chat_id, url, sec)
            status = "å•Ÿç”¨" if created else "æ›´æ–°"
            msg = f"ä½ çš„ä»»å‹™ï¼š\n{tid}ï½œ{status}ï½œ{sec}s\n{canonicalize_url(url)}"
            return [TextSendMessage(text=msg)] if HAS_LINE else [msg]

        if cmd == "/unwatch" and len(parts) >= 2:
            ok = fs_disable(chat_id, parts[1].strip())
            msg = "å·²åœç”¨" if ok else "æ‰¾ä¸åˆ°è©²ä»»å‹™"
            return [TextSendMessage(text=msg)] if HAS_LINE else [msg]

        if cmd == "/list":
            mode = "on"
            if len(parts) >= 2:
                t = parts[1].lower()
                if t in ("all", "off"):
                    mode = t
            rows = fs_list(chat_id, show="off" if mode=="off" else ("all" if mode=="all" else "on"))
            if not rows:
                out = "ï¼ˆæ²’æœ‰ä»»å‹™ï¼‰"
            else:
                lines = ["ä½ çš„ä»»å‹™ï¼š"]
                for r in rows:
                    state = "å•Ÿç”¨" if r.get("enabled") else "åœç”¨"
                    lines.append(f"{r['id']}ï½œ{state}ï½œ{r.get('period')}s\n{r.get('url')}")
                out = "\n\n".join(lines)
            return [TextSendMessage(text=out)] if HAS_LINE else [out]

        if cmd == "/check" and len(parts) >= 2:
            target = parts[1].strip()
            if target.lower().startswith("http"):
                url = target
            else:
                doc = fs_get_task_by_id(chat_id, target)
                if not doc:
                    msg = "æ‰¾ä¸åˆ°è©²ä»»å‹™ ID"
                    return [TextSendMessage(text=msg)] if HAS_LINE else [msg]
                url = doc.to_dict().get("url")
            res = probe(url)
            msgs = []
            if HAS_LINE and res.get("image", LOGO) and res["image"] != LOGO:
                msgs.append(ImageSendMessage(original_content_url=res["image"], preview_image_url=res["image"]))
            text_out = fmt_result_text(res)
            msgs.append(TextSendMessage(text=text_out) if HAS_LINE else text_out)
            return msgs

        if cmd == "/probe" and len(parts) >= 2:
            url = parts[1].strip()
            res = probe(url)
            out = json.dumps(res, ensure_ascii=False)
            return [TextSendMessage(text=out)] if HAS_LINE else [out]

        return [TextSendMessage(text=HELP)] if HAS_LINE else [HELP]
    except Exception as e:
        app.logger.error(f"handle_command error: {e}\n{traceback.format_exc()}")
        msg = "æŒ‡ä»¤è™•ç†ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
        return [TextSendMessage(text=msg)] if HAS_LINE else [msg]

# ============= Webhook / Scheduler / Diag =============
@app.route("/webhook", methods=["POST"])
def webhook():
    if not (HAS_LINE and handler):
        # æ²’æœ‰ handler ä¹Ÿå› 200ï¼Œé¿å… LINE é‡è©¦é€£ç™¼
        app.logger.warning("Webhook invoked but handler not ready")
        return "OK", 200
    signature = request.headers.get("X-Line-Signature", "")
    body = request.get_data(as_text=True)
    try:
        handler.handle(body, signature)
    except InvalidSignatureError:
        app.logger.warning("InvalidSignature on /webhook")
        abort(400)
    return "OK"

# åªæœ‰åœ¨ handler å­˜åœ¨æ™‚æ‰æ› decoratorï¼Œé¿å…è¼‰å…¥æœŸå ±éŒ¯
if HAS_LINE and handler:
    @handler.add(MessageEvent, message=TextMessage)
    def on_message(ev):
        text = ev.message.text.strip()
        chat = source_id(ev)
        msgs = handle_command(text, chat)
        # SDK éœ€è¦ list[SendMessage]
        if isinstance(msgs, list) and msgs and not isinstance(msgs[0], str):
            line_bot_api.reply_message(ev.reply_token, msgs)
        else:
            # ç†è«–ä¸Šä¸æœƒé€²ä¾†ï¼›ä¿åº•
            line_bot_api.reply_message(ev.reply_token, [TextSendMessage(text=str(msgs))])

@app.route("/cron/tick", methods=["GET"])
def cron_tick():
    start = time.time()
    resp = {"ok": True, "processed": 0, "skipped": 0, "errors": []}
    try:
        if not FS_OK:
            resp["ok"] = False
            resp["errors"].append("No Firestore client")
            return jsonify(resp), 200

        now = datetime.now(timezone.utc)

        try:
            docs = list(fs_client.collection(COL).where("enabled", "==", True).stream())
        except Exception as e:
            app.logger.error(f"[tick] list watchers failed: {e}")
            resp["ok"] = False
            resp["errors"].append(f"list failed: {e}")
            return jsonify(resp), 200

        handled = 0
        for d in docs:
            if (time.time() - start) > TICK_SOFT_DEADLINE_SEC:
                resp["errors"].append("soft-deadline reached; remaining will run next tick")
                break
            if handled >= MAX_PER_TICK:
                resp["errors"].append("max-per-tick reached; remaining will run next tick")
                break

            r = d.to_dict()
            period = int(r.get("period", DEFAULT_PERIOD_SEC))
            next_run_at = r.get("next_run_at") or (now - timedelta(seconds=1))
            if now < next_run_at:
                resp["skipped"] += 1
                continue

            url = r.get("url")
            try:
                res = probe(url)
            except Exception as e:
                app.logger.error(f"[tick] probe error for {url}: {e}")
                res = {"ok": False, "msg": f"probe error: {e}", "sig": "NA", "url": url}

            try:
                fs_client.collection(COL).document(d.id).update({
                    "last_sig": res.get("sig", "NA"),
                    "last_total": res.get("total", 0),
                    "last_ok": bool(res.get("ok", False)),
                    "updated_at": now,
                    "next_run_at": now + timedelta(seconds=period),
                })
            except Exception as e:
                app.logger.error(f"[tick] update doc error: {e}")
                resp["errors"].append(f"update error: {e}")

            changed = (res.get("sig", "NA") != r.get("last_sig", ""))
            if ALWAYS_NOTIFY or changed:
                try:
                    text_out = fmt_result_text(res)
                    img = res.get("image", "")
                    chat_id = r.get("chat_id")
                    if img and img != LOGO:
                        send_image(chat_id, img)
                    send_text(chat_id, text_out)
                except Exception as e:
                    app.logger.error(f"[tick] notify error: {e}")
                    resp["errors"].append(f"notify error: {e}")

            handled += 1
            resp["processed"] += 1

        app.logger.info(f"[tick] processed={resp['processed']} skipped={resp['skipped']} "
                        f"errors={len(resp['errors'])} duration={time.time()-start:.1f}s")
        return jsonify(resp), 200

    except Exception as e:
        app.logger.error(f"[tick] fatal: {e}\n{traceback.format_exc()}")
        resp["ok"] = False
        resp["errors"].append(str(e))
        return jsonify(resp), 200

@app.route("/diag", methods=["GET"])
def diag():
    url = request.args.get("url", "").strip()
    if not url:
        return jsonify({"ok": False, "msg": "missing url"}), 400
    try:
        res = probe(url)
        return jsonify(res), 200
    except Exception as e:
        return jsonify({"ok": False, "msg": str(e)}), 500

@app.route("/healthz", methods=["GET"])
def healthz():
    return "ok", 200

# æ–¹ä¾¿ç›´æ¥ç”¨ GET æ¸¬ /checkï¼ˆä¸ç¶“ LINEï¼‰
@app.route("/check", methods=["GET"])
def http_check_once():
    url = request.args.get("url", "").strip()
    if not url:
        return jsonify({"ok": False, "msg": "provide ?url=<ibon url>"}), 400
    res = probe(url)
    return jsonify(res), 200

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", "8080")))